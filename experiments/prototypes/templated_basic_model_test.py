# -*- coding: utf-8 -*-
"""
í…œí”Œë¦¿ ê¸°ë°˜ ê¸°ë³¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ - ExperimentRegistry ì‹œìŠ¤í…œ ì‹¤ì¦
ê¸°ì¡´ basic_model_test.pyì˜ í…œí”Œë¦¿í™” ë²„ì „

BEFORE: í•˜ë“œì½”ë”©ëœ ë§¤ê°œë³€ìˆ˜ì™€ ì„¤ì •
AFTER: config/experiments.yaml ê¸°ë°˜ ì¤‘ì•™ì§‘ì¤‘ì‹ ì„¤ì • ê´€ë¦¬
"""

import sys
import time
import json
from typing import List, Dict, Any
from dataclasses import dataclass
sys.path.append('.')

# Registry ì‹œìŠ¤í…œë“¤ ì‚¬ìš©
from src.registry.model_registry import get_model_registry
from src.registry.experiment_registry import get_experiment_registry, ExperimentConfig

@dataclass 
class TemplatedResult:
    """í…œí”Œë¦¿ ê¸°ë°˜ ì‹¤í—˜ ê²°ê³¼"""
    method: str
    question: str
    expected: str
    predicted: str
    correct: bool
    tokens: int
    time_ms: int
    experiment_config: str
    metadata: Dict[str, Any]

class TemplatedModelTester:
    """í…œí”Œë¦¿ ê¸°ë°˜ ëª¨ë¸ í…ŒìŠ¤í„° - ì™„ì „í•œ ì„¤ì • ë¶„ë¦¬"""
    
    def __init__(self, environment: str = "development"):
        print(f">>> í…œí”Œë¦¿ ê¸°ë°˜ ëª¨ë¸ í…ŒìŠ¤í„° ì´ˆê¸°í™” - í™˜ê²½: {environment}")
        
        # Model Registryì™€ Experiment Registry í†µí•© ì‚¬ìš©
        self.model_registry = get_model_registry(environment)
        self.experiment_registry = get_experiment_registry(environment)
        
        # ì‹¤í—˜ ì„¤ì • ë¡œë“œ (í•˜ë“œì½”ë”© ì™„ì „ ì œê±°!)
        self.experiment_config = self.experiment_registry.get_experiment_config("basic_model_test")
        
        # ì„¤ì • ê²€ì¦
        warnings = self.experiment_registry.validate_config("basic_model_test")
        if warnings:
            print(">>> ì„¤ì • ê²½ê³ ì‚¬í•­:")
            for warning in warnings:
                print(f"  âš ï¸ {warning}")
        
        # ì‹¤í—˜ ì„¤ì •ì— ë”°ë¥¸ ëª¨ë¸ ë¡œë”©
        self.models = {}
        for role in self.experiment_config.roles_required:
            model_name = self.model_registry.get_model_name(role)
            self.models[model_name] = self.model_registry.get_model(role)
        
        # ì„¤ì • ì •ë³´ ì¶œë ¥
        print(f"  ì‹¤í—˜ ì„¤ì •: {self.experiment_config.name}")
        print(f"  ì„¤ëª…: {self.experiment_config.description}")
        print(f"  ì‚¬ìš© ëª¨ë¸: {list(self.models.keys())}")
        print(f"  ë©”íŠ¸ë¦­: {self.experiment_config.metrics}")
        print(">>> í…œí”Œë¦¿ ê¸°ë°˜ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def get_test_categories(self):
        """ì‹¤í—˜ ì„¤ì •ì—ì„œ í…ŒìŠ¤íŠ¸ ì¹´í…Œê³ ë¦¬ ë¡œë“œ"""
        # ì‹¤í—˜ ì„¤ì •ì—ì„œ test_categories ê°€ì ¸ì˜¤ê¸° (í•˜ë“œì½”ë”© ì œê±°)
        return self.experiment_config.metadata.get('test_categories', [
            "ê¸°ë³¸ ì‚¬ì¹™ì—°ì‚°",
            "ê°„ë‹¨í•œ Word Problem", 
            "ë‹¤ë‹¨ê³„ ê³„ì‚°",
            "í•œêµ­ì–´ ì´í•´ë ¥"
        ])
    
    def get_test_cases(self, category: str) -> List[Dict[str, str]]:
        """ì¹´í…Œê³ ë¦¬ë³„ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¡œë“œ (ì¶”í›„ ì™¸ë¶€ íŒŒì¼ë¡œ ë¶„ë¦¬ ê°€ëŠ¥)"""
        test_data = {
            "ê¸°ë³¸ ì‚¬ì¹™ì—°ì‚°": [
                {"question": "What is 15 - 7?", "expected": "8"},
                {"question": "What is 25 * 0.20?", "expected": "5"},
                {"question": "What is 25 - 5?", "expected": "20"},
            ],
            "ê°„ë‹¨í•œ Word Problem": [
                {"question": "Sarah has 15 apples. She gives away 7. How many are left?", "expected": "8"},
                {"question": "A shirt costs $25. With 20% discount, what is the final price?", "expected": "20"},
            ],
            "ë‹¤ë‹¨ê³„ ê³„ì‚°": [
                {"question": "Rectangle is 12m by 8m. Perimeter is?", "expected": "40"},
                {"question": "Two trains 240 miles apart, speeds 60 and 80 mph. Meeting time in hours?", "expected": "1.714"},
            ],
            "í•œêµ­ì–´ ì´í•´ë ¥": [
                {"question": "15ì—ì„œ 7ì„ ë¹¼ë©´?", "expected": "8"},
                {"question": "ì‚¬ê³¼ 15ê°œì—ì„œ 7ê°œ ì£¼ë©´ ëª‡ê°œ ë‚¨ì•„?", "expected": "8"},
            ]
        }
        return test_data.get(category, [])
    
    def extract_answer(self, response_text: str) -> str:
        """ë‹µë³€ì—ì„œ ìˆ«ì ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
        import re
        
        patterns = [
            r'ë‹µ[ì€ëŠ”]?\s*([+-]?\d+(?:\.\d+)?)',
            r'answer[is:]*\s*([+-]?\d+(?:\.\d+)?)',
            r'result[is:]*\s*([+-]?\d+(?:\.\d+)?)',
            r'=\s*([+-]?\d+(?:\.\d+)?)',
            r'([+-]?\d+(?:\.\d+)?)\s*$',
            r'([+-]?\d+(?:\.\d+)?)\s*[ê°œë‹¬ëŸ¬ì›]',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, response_text, re.IGNORECASE)
            if match:
                return match.group(1)
        
        numbers = re.findall(r'([+-]?\d+(?:\.\d+)?)', response_text)
        if numbers:
            return numbers[-1]
        
        return ""
    
    def is_correct(self, predicted: str, expected: str) -> bool:
        """ì •ë‹µ ì—¬ë¶€ í™•ì¸ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
        if not predicted:
            return False
        
        try:
            pred_num = float(predicted)
            exp_num = float(expected)
            return abs(pred_num - exp_num) < 0.01
        except:
            return predicted.strip().lower() == expected.strip().lower()
    
    def test_model(self, model_name: str, model, test_cases: List[Dict], test_type: str):
        """ê°œë³„ ëª¨ë¸ í…ŒìŠ¤íŠ¸ - í…œí”Œë¦¿ ë§¤ê°œë³€ìˆ˜ ì‚¬ìš©"""
        print(f"\n=== {model_name} - {test_type} ===")
        
        # ì‹¤í—˜ ì„¤ì •ì—ì„œ ìƒì„± ë§¤ê°œë³€ìˆ˜ ê°€ì ¸ì˜¤ê¸° (í•˜ë“œì½”ë”© ì œê±°!)
        gen_params = self.experiment_config.get_generation_params()
        prompts_per_model = self.experiment_config.generation_params.get('prompts_per_model', 4)
        
        results = []
        for i, test in enumerate(test_cases):
            question = test["question"]
            expected = test["expected"]
            
            # í…œí”Œë¦¿ì—ì„œ ì •ì˜ëœ ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼
            prompts = [
                f"{question}",
                f"Question: {question}\nAnswer:",
                f"{question}\n\nProvide only the numerical answer:",
                f"Solve: {question}\nFinal answer:"
            ]
            
            best_result = None
            for prompt_idx, prompt in enumerate(prompts[:prompts_per_model]):
                try:
                    start_time = time.time()
                    
                    # í…œí”Œë¦¿ ë§¤ê°œë³€ìˆ˜ ì‚¬ìš© (í•˜ë“œì½”ë”© ì œê±°!)
                    response = model.generate(
                        prompt, 
                        temperature=gen_params.temperature,
                        max_tokens=gen_params.max_tokens
                    )
                    
                    if isinstance(response, dict):
                        answer_text = response.get("response", "").strip()
                    else:
                        answer_text = str(response).strip()
                    
                    predicted = self.extract_answer(answer_text)
                    correct = self.is_correct(predicted, expected)
                    time_ms = int((time.time() - start_time) * 1000)
                    
                    result = {
                        "question": question,
                        "expected": expected,
                        "predicted": predicted,
                        "correct": correct,
                        "full_response": answer_text,
                        "prompt_type": prompt_idx,
                        "time_ms": time_ms,
                        "generation_params": gen_params.to_dict()
                    }
                    
                    if correct or best_result is None:
                        best_result = result
                    
                    if correct:
                        break
                        
                except Exception as e:
                    result = {
                        "question": question,
                        "expected": expected,
                        "predicted": "",
                        "correct": False,
                        "full_response": f"ERROR: {str(e)}",
                        "prompt_type": prompt_idx,
                        "time_ms": 0,
                        "generation_params": gen_params.to_dict()
                    }
                    if best_result is None:
                        best_result = result
            
            results.append(best_result)
            
            # ì‹¤ì‹œê°„ ì¶œë ¥
            status = "âœ…" if best_result["correct"] else "âŒ"
            print(f"  {status} {question[:40]}... â†’ {best_result['predicted']} (expected: {expected})")
            if not best_result["correct"]:
                print(f"      ì „ì²´ ì‘ë‹µ: {best_result['full_response'][:80]}...")
        
        # ìš”ì•½ í†µê³„
        correct_count = sum(1 for r in results if r["correct"])
        accuracy = correct_count / len(results)
        avg_time = sum(r["time_ms"] for r in results) / len(results)
        
        print(f"  ğŸ“Š ì •í™•ë„: {accuracy:.1%} ({correct_count}/{len(results)})")
        print(f"  â±ï¸  í‰ê·  ì‹œê°„: {avg_time:.0f}ms")
        print(f"  ğŸ›ï¸  ì‚¬ìš©ëœ ë§¤ê°œë³€ìˆ˜: temp={gen_params.temperature}, tokens={gen_params.max_tokens}")
        
        return results

def run_templated_experiment(environment: str = "development"):
    """í…œí”Œë¦¿ ê¸°ë°˜ ì‹¤í—˜ ì‹¤í–‰"""
    print("=" * 80)
    print(">>> í…œí”Œë¦¿ ê¸°ë°˜ ê¸°ë³¸ ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    print(f">>> í™˜ê²½: {environment}")
    print("=" * 80)
    
    tester = TemplatedModelTester(environment)
    
    # ì‹¤í—˜ ì„¤ì •ì—ì„œ í…ŒìŠ¤íŠ¸ ì¹´í…Œê³ ë¦¬ ë¡œë“œ
    test_categories = tester.get_test_categories()
    
    all_results = {}
    
    # ê° ëª¨ë¸ë³„ë¡œ ëª¨ë“  í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
    for model_name, model in tester.models.items():
        print(f"\nğŸ¤– {model_name} í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        all_results[model_name] = {}
        
        for category in test_categories:
            test_cases = tester.get_test_cases(category)
            if test_cases:  # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ê°€ ìˆëŠ” ì¹´í…Œê³ ë¦¬ë§Œ ì‹¤í–‰
                results = tester.test_model(model_name, model, test_cases, category)
                all_results[model_name][category] = results
    
    # ê²°ê³¼ ë¶„ì„ ë° ì €ì¥
    analyze_templated_results(all_results, tester.experiment_config)
    save_templated_results(all_results, tester.experiment_config)
    
    return all_results

def analyze_templated_results(results: Dict, config: ExperimentConfig):
    """í…œí”Œë¦¿ ê¸°ë°˜ ê²°ê³¼ ë¶„ì„"""
    print(f"\n" + "=" * 80)
    print("ğŸ“Š í…œí”Œë¦¿ ê¸°ë°˜ ì‹¤í—˜ ê²°ê³¼ ë¶„ì„")
    print("=" * 80)
    
    print(f"ì‹¤í—˜ ì„¤ì •: {config.name} ({config.description})")
    print(f"í™˜ê²½: {config.environment}")
    print(f"ì„¤ì • í•´ì‹œ: {config.metadata.get('config_hash', 'unknown')}")
    
    # ê¸°ì¡´ ë¶„ì„ ë¡œì§ + í…œí”Œë¦¿ ë©”íƒ€ë°ì´í„°
    for model_name in results.keys():
        total_correct = 0
        total_tests = 0
        total_time = 0
        
        for test_type, test_results in results[model_name].items():
            for result in test_results:
                total_correct += int(result["correct"])
                total_tests += 1
                total_time += result["time_ms"]
        
        accuracy = total_correct / total_tests if total_tests > 0 else 0
        avg_time = total_time / total_tests if total_tests > 0 else 0
        
        print(f"  {model_name}: {accuracy:.1%} ({total_correct}/{total_tests}) - {avg_time:.0f}ms")

def save_templated_results(results: Dict, config: ExperimentConfig):
    """í…œí”Œë¦¿ ê¸°ë°˜ ê²°ê³¼ ì €ì¥"""
    # ì‹¤í—˜ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ ì¶œë ¥ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    output_config = get_experiment_registry(config.environment).get_output_config()
    
    timestamp = int(time.time())
    filename_template = output_config.get('filename_template', '{experiment_type}_{environment}_{timestamp}')
    filename = filename_template.format(
        experiment_type=config.name,
        environment=config.environment,
        timestamp=timestamp
    )
    
    full_filename = f"results/{filename}.json"
    
    # í…œí”Œë¦¿ ë©”íƒ€ë°ì´í„° í¬í•¨ëœ ê²°ê³¼ ì €ì¥
    serializable_results = {
        "experiment_metadata": {
            "template_name": config.name,
            "description": config.description,
            "environment": config.environment,
            "timestamp": timestamp,
            "config_hash": config.metadata.get('config_hash'),
            "roles_used": config.roles_required,
            "metrics": config.metrics,
            "generation_params_used": config.generation_params
        },
        "results": results
    }
    
    try:
        import os
        os.makedirs("results", exist_ok=True)
        
        with open(full_filename, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ í…œí”Œë¦¿ ê¸°ë°˜ ê²°ê³¼ ì €ì¥: {full_filename}")
        print(f"ğŸ“Š ë©”íƒ€ë°ì´í„° í¬í•¨í•˜ì—¬ ì¬í˜„ ê°€ëŠ¥í•œ ì‹¤í—˜ ê²°ê³¼ ìƒì„±")
        
    except Exception as e:
        print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    print(">>> í…œí”Œë¦¿ ê¸°ë°˜ ê¸°ë³¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    print(">>> ExperimentRegistry ì‹œìŠ¤í…œ ì‹¤ì¦ ì‹¤í—˜")
    
    # í™˜ê²½ ì„ íƒ
    print("\n>>> í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„ íƒ:")
    print("  1. development (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)")
    print("  2. test (ì¤‘ê°„ ì„±ëŠ¥)")
    print("  3. production (ì™„ì „í•œ í…ŒìŠ¤íŠ¸)")
    
    choice = input("\ní™˜ê²½ ì„ íƒ (1-3, ê¸°ë³¸ê°’=1): ").strip() or "1"
    environments = {"1": "development", "2": "test", "3": "production"}
    environment = environments.get(choice, "development")
    
    print(f"\n>>> {environment} í™˜ê²½ìœ¼ë¡œ í…œí”Œë¦¿ ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    results = run_templated_experiment(environment)
    
    print(f"\n>>> í…œí”Œë¦¿ ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f">>> config/experiments.yamlì˜ ì„¤ì •ì´ ì„±ê³µì ìœ¼ë¡œ ì ìš©ë¨")
    print(f">>> í•˜ë“œì½”ë”©ëœ ë§¤ê°œë³€ìˆ˜ ì™„ì „ ì œê±° ë‹¬ì„±!")