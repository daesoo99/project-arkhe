#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Project Arkhƒì - 2-Stage Pipeline Comparison Experiment
2Îã®Í≥Ñ ÌååÏù¥ÌîÑÎùºÏù∏ (Draft ‚Üí Judge) ÏÑ±Îä• ÎπÑÍµê

Î™©Ï†Å: Review Îã®Í≥Ñ Ï†úÍ±∞Í∞Ä Ìö®Ïú®ÏÑ± Í∞úÏÑ† ÏóÜÏù¥ ÌíàÏßà Ïú†ÏßÄ Í∞ÄÎä•ÌïúÏßÄ Í≤ÄÏ¶ù
"""

import sys
import os
import json
import time
from pathlib import Path
from dataclasses import dataclass
from typing import List, Dict, Any

sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from orchestrator.isolation_pipeline import IsolationPipeline, IsolationLevel
from llm.simple_llm import create_llm_auto

@dataclass
class TwoStageResult:
    """2Îã®Í≥Ñ Ïã§Ìóò Í≤∞Í≥º"""
    method_name: str
    problem_id: str
    query: str
    expected_answer: str
    
    final_answer: str
    total_tokens: int
    total_time_ms: int
    accuracy_score: float
    
    # Îã®Í≥ÑÎ≥Ñ Ï†ïÎ≥¥
    draft_samples: List[str]
    judge_samples: List[str]
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "method_name": self.method_name,
            "problem_id": self.problem_id,
            "query": self.query,
            "expected_answer": self.expected_answer,
            "final_answer": self.final_answer,
            "total_tokens": self.total_tokens,
            "total_time_ms": self.total_time_ms,
            "accuracy_score": float(self.accuracy_score),
            "draft_samples": self.draft_samples,
            "judge_samples": self.judge_samples
        }

class TwoStagePipeline:
    """2Îã®Í≥Ñ ÌååÏù¥ÌîÑÎùºÏù∏: Draft ‚Üí Judge"""
    
    def __init__(self):
        pass
    
    def run_experiment(self, query: str, expected_answer: str, 
                      question_id: str, llm_factory) -> TwoStageResult:
        """2Îã®Í≥Ñ Ïã§Ìóò Ïã§Ìñâ"""
        
        print(f"\n*** 2-STAGE PIPELINE EXPERIMENT ***")
        print(f"Query: {query[:100]}...")
        
        start_time = time.time()
        
        # Stage 1: Draft Generation (qwen2:0.5b √ó 3)
        draft_samples = self._run_draft_stage(llm_factory, query)
        
        # Stage 2: Final Judge (llama3:8b √ó 1) - Skip Review
        judge_samples = self._run_judge_stage(llm_factory, query, draft_samples)
        
        # ÏµúÏ¢Ö ÎãµÎ≥Ä
        final_answer = judge_samples[0] if judge_samples else "No answer"
        accuracy_score = self._calculate_accuracy_score(final_answer, expected_answer)
        
        # ÌÜ†ÌÅ∞ Í≥ÑÏÇ∞
        total_tokens = self._calculate_tokens(query, draft_samples, judge_samples)
        total_time = int((time.time() - start_time) * 1000)
        
        return TwoStageResult(
            method_name="2-Stage-Pipeline",
            problem_id=question_id,
            query=query,
            expected_answer=expected_answer,
            final_answer=final_answer,
            total_tokens=total_tokens,
            total_time_ms=total_time,
            accuracy_score=accuracy_score,
            draft_samples=draft_samples,
            judge_samples=judge_samples
        )
    
    def _run_draft_stage(self, llm_factory, query: str) -> List[str]:
        """Draft Îã®Í≥Ñ Ïã§Ìñâ"""
        print(f"  [1/2] Draft stage - 3 samples (qwen2:0.5b)")
        
        llm = llm_factory("qwen2:0.5b")
        prompt = f"Answer this question concisely: {query}"
        
        samples = []
        for i in range(3):
            response = llm.generate(prompt, temperature=0.8)
            text = response.get("response", str(response)) if isinstance(response, dict) else str(response)
            samples.append(text)
        
        return samples
    
    def _run_judge_stage(self, llm_factory, query: str, draft_samples: List[str]) -> List[str]:
        """Judge Îã®Í≥Ñ Ïã§Ìñâ (Review Í±¥ÎÑàÎõ∞Í∏∞)"""
        print(f"  [2/2] Judge stage - 1 sample (llama3:8b) - DIRECT FROM DRAFT")
        
        llm = llm_factory("llama3:8b")
        
        # Draft Ï†ïÎ≥¥Î•º ÏßÅÏ†ë JudgeÏóê Ï†ÑÎã¨
        prompt = f"""Provide the final, highest quality answer by synthesizing these draft responses:

Question: {query}
Draft answers: {' | '.join(draft_samples)}

Final answer:"""
        
        response = llm.generate(prompt, temperature=0.4)
        text = response.get("response", str(response)) if isinstance(response, dict) else str(response)
        
        return [text]
    
    def _calculate_tokens(self, query: str, draft_samples: List[str], 
                         judge_samples: List[str]) -> int:
        """ÌÜ†ÌÅ∞ Í≥ÑÏÇ∞"""
        try:
            import tiktoken
            encoder = tiktoken.encoding_for_model("gpt-4")
            
            total_tokens = 0
            
            # Draft Îã®Í≥Ñ
            draft_input = f"Answer this question concisely: {query}"
            total_tokens += len(encoder.encode(draft_input)) * 3  # 3Í∞ú ÏÉòÌîå
            for sample in draft_samples:
                total_tokens += len(encoder.encode(sample))
            
            # Judge Îã®Í≥Ñ (Review Í±¥ÎÑàÎõ∞Í∏∞)
            judge_input = f"Provide the final, highest quality answer by synthesizing these draft responses: Question: {query} Draft answers: {' | '.join(draft_samples)} Final answer:"
            total_tokens += len(encoder.encode(judge_input)) * 1  # 1Í∞ú ÏÉòÌîå
            for sample in judge_samples:
                total_tokens += len(encoder.encode(sample))
            
            return total_tokens
            
        except ImportError:
            # Ìè¥Î∞±: Îã®Ïñ¥ Ïàò Í∏∞Î∞ò
            return sum(len(sample.split()) for samples in [draft_samples, judge_samples] 
                      for sample in samples)
    
    def _calculate_accuracy_score(self, final_answer: str, expected_answer: str) -> float:
        """Ï†ïÎãµÍ≥ºÏùò ÏùºÏπòÎèÑ Í≥ÑÏÇ∞"""
        if not expected_answer or not final_answer:
            return 0.0
        
        final_lower = final_answer.lower()
        expected_lower = expected_answer.lower()
        
        # ÏôÑÏ†Ñ ÏùºÏπò
        if expected_lower in final_lower:
            return 1.0
        
        # Î∂ÄÎ∂Ñ ÏùºÏπò (Îã®Ïñ¥ Îã®ÏúÑ)
        expected_words = set(expected_lower.split())
        final_words = set(final_lower.split())
        
        if expected_words and expected_words.intersection(final_words):
            return len(expected_words.intersection(final_words)) / len(expected_words)
        
        return 0.0

class TwoStageComparisonRunner:
    """2Îã®Í≥Ñ ÌååÏù¥ÌîÑÎùºÏù∏ ÎπÑÍµê Ïã§Ìóò Îü¨ÎÑà"""
    
    def __init__(self):
        self.load_test_questions()
    
    def load_test_questions(self):
        """ÌëúÏ§Ä Î≤§ÏπòÎßàÌÅ¨ Î¨∏Ï†ú Î°úÎìú"""
        try:
            from datasets.standard_benchmarks import StandardBenchmarkLoader
            loader = StandardBenchmarkLoader()
            questions = loader.get_questions(count=20, categories=["math", "knowledge", "coding"])
            
            self.problems = []
            for q in questions:
                self.problems.append({
                    "id": q.id,
                    "query": q.query,
                    "expected": q.expected_answer
                })
            
            print(f"[+] Loaded {len(self.problems)} standard benchmark questions")
            
        except Exception as e:
            # Ìè¥Î∞±: Í∞ÑÎã®Ìïú ÏßàÎ¨∏Îì§
            print(f"[!] Using fallback questions: {e}")
            self.problems = [
                {"id": "simple_geo", "query": "ÎåÄÌïúÎØºÍµ≠Ïùò ÏàòÎèÑÎäî?", "expected": "ÏÑúÏö∏"},
                {"id": "simple_math", "query": "What is 2 + 2?", "expected": "4"},
                {"id": "medium_prog", "query": "PythonÏóêÏÑú Î¶¨Ïä§Ìä∏Î•º Ï†ïÎ†¨ÌïòÎäî Î©îÏÑúÎìúÎäî?", "expected": "sort"},
                {"id": "medium_net", "query": "HTTPÏùò Í∏∞Î≥∏ Ìè¨Ìä∏ Î≤àÌò∏Îäî?", "expected": "80"},
                {"id": "hard_algo", "query": "ÏãúÍ∞Ñ Î≥µÏû°ÎèÑ O(n log n)Ïù∏ Ï†ïÎ†¨ ÏïåÍ≥†Î¶¨Ï¶òÏùÄ?", "expected": "merge sort"}
            ]
    
    def run_full_comparison(self) -> Dict[str, List[TwoStageResult]]:
        """Ï†ÑÏ≤¥ ÎπÑÍµê Ïã§Ìóò"""
        
        print("=" * 80)
        print("*** 2-STAGE vs 3-STAGE vs SINGLE COMPARISON ***")
        print("Review Îã®Í≥Ñ Ï†úÍ±∞ Ìö®Í≥º Í≤ÄÏ¶ù")
        print("=" * 80)
        
        # ÌÖåÏä§Ìä∏Ìï† Î∞©Î≤ïÎì§
        methods = [
            ("2-Stage-Pipeline", self._run_2stage),
            ("3-Stage-NONE", self._run_3stage_none),
            ("Single-llama3:8b", lambda p: self._run_single_model(p, "llama3:8b"))
        ]
        
        all_results = {}
        
        for method_name, method_func in methods:
            print(f"\n[*] Testing {method_name}...")
            all_results[method_name] = []
            
            for i, problem in enumerate(self.problems, 1):
                print(f"\n  [{i}/{len(self.problems)}] Problem: {problem['id']}")
                print(f"  Query: {problem['query'][:100]}...")
                
                try:
                    result = method_func(problem)
                    all_results[method_name].append(result)
                    
                    print(f"    ‚Üí Tokens: {result.total_tokens}, Time: {result.total_time_ms}ms")
                    print(f"    ‚Üí Accuracy: {result.accuracy_score:.3f}")
                    
                except Exception as e:
                    print(f"    ‚Üí ERROR: {e}")
                
                time.sleep(1)  # Î™®Îç∏ Í≥ºÎ∂ÄÌïò Î∞©ÏßÄ
        
        # Í≤∞Í≥º Ï†ÄÏû•
        self._save_results(all_results)
        
        # ÏµúÏ¢Ö Î∂ÑÏÑù
        self._print_final_analysis(all_results)
        
        return all_results
    
    def _run_2stage(self, problem: Dict[str, str]) -> TwoStageResult:
        """2Îã®Í≥Ñ ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ"""
        pipeline = TwoStagePipeline()
        
        result = pipeline.run_experiment(
            query=problem['query'],
            expected_answer=problem['expected'],
            question_id=problem['id'],
            llm_factory=create_llm_auto
        )
        
        return result
    
    def _run_3stage_none(self, problem: Dict[str, str]) -> TwoStageResult:
        """3Îã®Í≥Ñ ÌååÏù¥ÌîÑÎùºÏù∏ (NONE) Ïã§Ìñâ"""
        pipeline = IsolationPipeline(IsolationLevel.NONE, k_samples=3)
        
        isolation_result = pipeline.run_experiment(
            query=problem['query'],
            expected_answer=problem['expected'],
            question_id=problem['id'],
            llm_factory=create_llm_auto
        )
        
        # TwoStageResult Ìè¨Îß∑ÏúºÎ°ú Î≥ÄÌôò
        return TwoStageResult(
            method_name="3-Stage-NONE",
            problem_id=problem['id'],
            query=problem['query'],
            expected_answer=problem['expected'],
            final_answer=isolation_result.final_answer,
            total_tokens=isolation_result.total_tokens,
            total_time_ms=isolation_result.total_time_ms,
            accuracy_score=isolation_result.accuracy_score,
            draft_samples=isolation_result.draft_samples,
            judge_samples=isolation_result.judge_samples
        )
    
    def _run_single_model(self, problem: Dict[str, str], model_name: str) -> TwoStageResult:
        """Îã®Ïùº Î™®Îç∏ Ïã§Ìñâ"""
        start_time = time.time()
        
        llm = create_llm_auto(model_name)
        prompt = f"Answer this question: {problem['query']}"
        
        try:
            response = llm.generate(prompt, temperature=0.3, max_tokens=1024)
            
            if isinstance(response, dict):
                final_answer = response.get("response", str(response))
            else:
                final_answer = str(response)
            
            # ÌÜ†ÌÅ∞ Í≥ÑÏÇ∞
            try:
                import tiktoken
                encoder = tiktoken.encoding_for_model("gpt-4")
                tokens = len(encoder.encode(prompt)) + len(encoder.encode(final_answer))
            except ImportError:
                tokens = len(prompt.split()) + len(final_answer.split())
            
            total_time = int((time.time() - start_time) * 1000)
            
            # Ï†ïÌôïÎèÑ Í≥ÑÏÇ∞
            final_lower = final_answer.lower()
            expected_lower = problem['expected'].lower()
            
            if expected_lower in final_lower:
                accuracy = 1.0
            else:
                expected_words = set(expected_lower.split())
                final_words = set(final_lower.split())
                if expected_words:
                    accuracy = len(expected_words.intersection(final_words)) / len(expected_words)
                else:
                    accuracy = 0.0
            
            return TwoStageResult(
                method_name=f"Single-{model_name}",
                problem_id=problem['id'],
                query=problem['query'],
                expected_answer=problem['expected'],
                final_answer=final_answer,
                total_tokens=tokens,
                total_time_ms=total_time,
                accuracy_score=accuracy,
                draft_samples=[],
                judge_samples=[final_answer]
            )
            
        except Exception as e:
            raise e
    
    def _save_results(self, results: Dict[str, List[TwoStageResult]]):
        """Í≤∞Í≥º Ï†ÄÏû•"""
        results_file = f"experiments/results/2stage_comparison_{int(time.time())}.json"
        Path(results_file).parent.mkdir(parents=True, exist_ok=True)
        
        # JSON ÏßÅÎ†¨Ìôî
        json_results = {}
        for method, result_list in results.items():
            json_results[method] = [r.to_dict() for r in result_list]
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(json_results, f, ensure_ascii=False, indent=2)
        
        print(f"\n[+] Results saved: {results_file}")
        return results_file
    
    def _print_final_analysis(self, results: Dict[str, List[TwoStageResult]]):
        """ÏµúÏ¢Ö Î∂ÑÏÑù Ï∂úÎ†•"""
        print("\n" + "=" * 80)
        print("*** 2-STAGE PIPELINE ANALYSIS ***")
        print("=" * 80)
        
        # Ï†ÑÏ≤¥ ÏöîÏïΩ
        print(f"\n{'Method':<20} {'Avg Accuracy':<12} {'Avg Tokens':<11} {'Avg Time(ms)':<12} {'Efficiency':<10}")
        print("-" * 70)
        
        method_summaries = {}
        
        for method, result_list in results.items():
            if not result_list:
                continue
                
            valid_results = [r for r in result_list if r.total_tokens > 0]
            if not valid_results:
                continue
            
            avg_accuracy = sum(r.accuracy_score for r in valid_results) / len(valid_results)
            avg_tokens = sum(r.total_tokens for r in valid_results) / len(valid_results)
            avg_time = sum(r.total_time_ms for r in valid_results) / len(valid_results)
            
            # Ìö®Ïú®ÏÑ± = Ï†ïÌôïÎèÑ / (ÌÜ†ÌÅ∞ / 100)
            efficiency = avg_accuracy / (avg_tokens / 100) if avg_tokens > 0 else 0
            
            method_summaries[method] = {
                "accuracy": avg_accuracy,
                "tokens": avg_tokens,
                "time": avg_time,
                "efficiency": efficiency
            }
            
            print(f"{method:<20} {avg_accuracy:<12.3f} {avg_tokens:<11.0f} {avg_time:<12.0f} {efficiency:<10.3f}")
        
        # ÌïµÏã¨ ÎπÑÍµê
        if "2-Stage-Pipeline" in method_summaries and "3-Stage-NONE" in method_summaries:
            two_stage = method_summaries["2-Stage-Pipeline"]
            three_stage = method_summaries["3-Stage-NONE"]
            
            print(f"\n{'='*20} 2-STAGE vs 3-STAGE {'='*20}")
            
            accuracy_diff = ((two_stage["accuracy"] - three_stage["accuracy"]) / three_stage["accuracy"]) * 100
            token_diff = ((two_stage["tokens"] - three_stage["tokens"]) / three_stage["tokens"]) * 100
            efficiency_diff = ((two_stage["efficiency"] - three_stage["efficiency"]) / three_stage["efficiency"]) * 100
            
            print(f"Accuracy Change (2-stage vs 3-stage): {accuracy_diff:+.1f}%")
            print(f"Token Cost Change: {token_diff:+.1f}%")
            print(f"Efficiency Change: {efficiency_diff:+.1f}%")
            
            # Í≤∞Î°†
            if efficiency_diff > 5:
                print("üèÜ 2-Stage WINS: Better efficiency with similar quality!")
            elif efficiency_diff > -5:
                print("ü§î MIXED RESULTS: Similar performance")
            else:
                print("üò∞ 3-Stage WINS: Quality loss not worth the savings")

def main():
    """Î©îÏù∏ Ïã§Ìñâ Ìï®Ïàò"""
    try:
        runner = TwoStageComparisonRunner()
        results = runner.run_full_comparison()
        
        print("\n" + "üéØ" * 30)
        print("*** 2-STAGE PIPELINE EXPERIMENT COMPLETED ***")
        print("Check detailed results in the saved JSON file.")
        
    except KeyboardInterrupt:
        print("\n[!] Experiment interrupted by user")
    except Exception as e:
        print(f"\n[!] Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()