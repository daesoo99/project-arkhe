#!/usr/bin/env python3
"""
Project Arkhƒì - Adaptive System Test
ÏÉàÎ°úÏö¥ Î¨∏Ï†úÎì§Ïóê ÎåÄÌïú ÏßÄÎä•Ìòï Î™®Îç∏ Î∞∞Ïπò ÌÖåÏä§Ìä∏
"""

import json
import time
import sys
import os
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from llm.simple_llm import create_llm_auto
from orchestrator.adaptive_system import AdaptiveOrchestrator
from utils.scorers import score_task

def load_adaptive_test_problems():
    """ÏÉàÎ°úÏö¥ ÌÖåÏä§Ìä∏ Î¨∏Ï†úÎì§ Î°úÎìú"""
    tasks_file = Path(__file__).parent.parent / "prompts" / "adaptive_test_problems.jsonl"
    tasks = []
    
    with open(tasks_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                tasks.append(json.loads(line))
    
    return tasks

def test_baseline_approach(task, llm_func):
    """Í∏∞Ï°¥ Î∞©Ïãù: Ìï≠ÏÉÅ llama3:8b Single ÏÇ¨Ïö©"""
    start_time = time.time()
    
    llm = llm_func("llama3:8b")
    response_dict = llm.generate(task["prompt"])
    response = response_dict.get('response', str(response_dict)) if isinstance(response_dict, dict) else str(response_dict)
    
    end_time = time.time()
    
    # ÌÜ†ÌÅ∞ Í≥ÑÏÇ∞ (Ï∂îÏ†ï)
    tokens = len(task["prompt"].split()) + len(response.split())
    
    return {
        "approach": "baseline",
        "model_used": "llama3:8b",
        "response": response,
        "time": end_time - start_time,
        "tokens": tokens,
        "decision_rationale": "Always use powerful single model"
    }

def test_adaptive_approach(task, llm_func, orchestrator):
    """ÏÉàÎ°úÏö¥ Î∞©Ïãù: Ï†ÅÏùëÌòï Î™®Îç∏ ÏÑ†ÌÉù"""
    start_time = time.time()
    
    result = orchestrator.execute_optimal_solution(task["prompt"], llm_func)
    
    end_time = time.time()
    result["time"] = end_time - start_time
    result["approach"] = "adaptive"
    
    return result

def calculate_metrics(result, task):
    """ÏÑ±Îä• Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞"""
    # Ï†ïÌôïÎèÑ ÌèâÍ∞Ä
    score = score_task(task.get("type", "reason"), task["answer"], result["response"])
    accuracy = score["score"]
    
    # Ìö®Ïú®ÏÑ± Í≥ÑÏÇ∞
    tokens = result.get("tokens", 0)
    efficiency = accuracy / (tokens / 100) if tokens > 0 else 0
    
    return {
        "accuracy": accuracy,
        "tokens": tokens,
        "time": result.get("time", 0),
        "efficiency": efficiency,
        "reasoning": score.get("reasoning", "")
    }

def main():
    """Î©îÏù∏ Ïã§Ìóò Ïã§Ìñâ"""
    print("Adaptive System Test: Smart Model Selection")
    print("=" * 60)
    
    # ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî
    orchestrator = AdaptiveOrchestrator()
    llm_func = create_llm_auto
    
    # ÌÖåÏä§Ìä∏ Î¨∏Ï†ú Î°úÎìú
    tasks = load_adaptive_test_problems()
    print(f"Loaded test problems: {len(tasks)}")
    
    results = []
    
    # Í∞Å Î¨∏Ï†úÏóê ÎåÄÌï¥ ÌÖåÏä§Ìä∏
    for i, task in enumerate(tasks):
        print(f"\n--- Test {i+1}: {task['id']} ---")
        print(f"Expected: {task['expected_config']}, Complexity: {task['expected_complexity']}")
        print(f"Problem: {task['prompt'][:80]}...")
        
        try:
            # Baseline ÌÖåÏä§Ìä∏ (Ìï≠ÏÉÅ llama3:8b)
            print("\\n  [BASELINE] Testing with llama3:8b...")
            baseline_result = test_baseline_approach(task, llm_func)
            baseline_metrics = calculate_metrics(baseline_result, task)
            
            print("\\n  [ADAPTIVE] Testing with smart selection...")
            # Adaptive ÌÖåÏä§Ìä∏ (ÏßÄÎä•Ìòï ÏÑ†ÌÉù)
            adaptive_result = test_adaptive_approach(task, llm_func, orchestrator)
            adaptive_metrics = calculate_metrics(adaptive_result, task)
            
            # Í≤∞Í≥º ÎπÑÍµê
            result = {
                "task": task,
                "baseline": {**baseline_result, **baseline_metrics},
                "adaptive": {**adaptive_result, **adaptive_metrics}
            }
            results.append(result)
            
            # Ï§ëÍ∞Ñ Í≤∞Í≥º Ï∂úÎ†•
            print(f"\\n  Results:")
            print(f"    Baseline:  accuracy {baseline_metrics['accuracy']:.2f}, tokens {baseline_metrics['tokens']}, time {baseline_metrics['time']:.1f}s")
            print(f"    Adaptive:  accuracy {adaptive_metrics['accuracy']:.2f}, tokens {adaptive_metrics['tokens']}, time {adaptive_metrics['time']:.1f}s")
            
            # Ìö®Ïú®ÏÑ± ÎπÑÍµê
            efficiency_gain = (adaptive_metrics['efficiency'] / baseline_metrics['efficiency'] - 1) * 100 if baseline_metrics['efficiency'] > 0 else 0
            accuracy_change = (adaptive_metrics['accuracy'] - baseline_metrics['accuracy']) * 100
            
            print(f"    Adaptive vs Baseline:")
            print(f"       Accuracy: {accuracy_change:+.1f}%")  
            print(f"       Efficiency: {efficiency_gain:+.1f}%")
            print(f"       Decision: {adaptive_result.get('configuration', {}).get('rationale', 'N/A')}")
            
        except Exception as e:
            print(f"    Error: {e}")
            continue
    
    # Ï†ÑÏ≤¥ Í≤∞Í≥º Î∂ÑÏÑù
    print("\\n" + "=" * 60)
    print("Overall Performance Analysis")
    print("=" * 60)
    
    if results:
        # Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ Î∂ÑÏÑù
        categories = {}
        for result in results:
            task_type = result["task"].get("type", "unknown")
            if task_type not in categories:
                categories[task_type] = {"baseline": [], "adaptive": []}
            
            categories[task_type]["baseline"].append(result["baseline"])
            categories[task_type]["adaptive"].append(result["adaptive"])
        
        print("\\nüìä Performance by Category:")
        
        overall_baseline = {"accuracy": 0, "tokens": 0, "time": 0, "efficiency": 0}
        overall_adaptive = {"accuracy": 0, "tokens": 0, "time": 0, "efficiency": 0}
        
        for category, data in categories.items():
            print(f"\\nüè∑Ô∏è  {category.upper()}:")
            
            # ÌèâÍ∑† Í≥ÑÏÇ∞
            baseline_avg = {
                "accuracy": sum(r["accuracy"] for r in data["baseline"]) / len(data["baseline"]),
                "tokens": sum(r["tokens"] for r in data["baseline"]) / len(data["baseline"]), 
                "time": sum(r["time"] for r in data["baseline"]) / len(data["baseline"]),
                "efficiency": sum(r["efficiency"] for r in data["baseline"]) / len(data["baseline"])
            }
            
            adaptive_avg = {
                "accuracy": sum(r["accuracy"] for r in data["adaptive"]) / len(data["adaptive"]),
                "tokens": sum(r["tokens"] for r in data["adaptive"]) / len(data["adaptive"]),
                "time": sum(r["time"] for r in data["adaptive"]) / len(data["adaptive"]), 
                "efficiency": sum(r["efficiency"] for r in data["adaptive"]) / len(data["adaptive"])
            }
            
            print(f"    Baseline:  acc {baseline_avg['accuracy']:.2f}, tok {baseline_avg['tokens']:.0f}, eff {baseline_avg['efficiency']:.3f}")
            print(f"    Adaptive:  acc {adaptive_avg['accuracy']:.2f}, tok {adaptive_avg['tokens']:.0f}, eff {adaptive_avg['efficiency']:.3f}")
            
            # Í∞úÏÑ†Ïú® Í≥ÑÏÇ∞
            acc_improvement = (adaptive_avg['accuracy'] - baseline_avg['accuracy']) * 100
            token_reduction = (1 - adaptive_avg['tokens'] / baseline_avg['tokens']) * 100 if baseline_avg['tokens'] > 0 else 0
            eff_improvement = (adaptive_avg['efficiency'] / baseline_avg['efficiency'] - 1) * 100 if baseline_avg['efficiency'] > 0 else 0
            
            print(f"    üìà Improvement: acc {acc_improvement:+.1f}%, tok {token_reduction:+.1f}%, eff {eff_improvement:+.1f}%")
            
            # Ï†ÑÏ≤¥ ÌèâÍ∑†Ïóê Ï∂îÍ∞Ä
            for key in overall_baseline:
                overall_baseline[key] += baseline_avg[key] * len(data["baseline"])
                overall_adaptive[key] += adaptive_avg[key] * len(data["adaptive"])
        
        # Ï†ÑÏ≤¥ ÌèâÍ∑† Í≥ÑÏÇ∞
        total_count = len(results)
        for key in overall_baseline:
            overall_baseline[key] /= total_count
            overall_adaptive[key] /= total_count
        
        print(f"\\nüéØ OVERALL PERFORMANCE:")
        print(f"  Baseline:  acc {overall_baseline['accuracy']:.2f}, tok {overall_baseline['tokens']:.0f}, eff {overall_baseline['efficiency']:.3f}")
        print(f"  Adaptive:  acc {overall_adaptive['accuracy']:.2f}, tok {overall_adaptive['tokens']:.0f}, eff {overall_adaptive['efficiency']:.3f}")
        
        # Ï†ÑÏ≤¥ Í∞úÏÑ†Ïú®
        overall_acc_improvement = (overall_adaptive['accuracy'] - overall_baseline['accuracy']) * 100
        overall_token_reduction = (1 - overall_adaptive['tokens'] / overall_baseline['tokens']) * 100
        overall_eff_improvement = (overall_adaptive['efficiency'] / overall_baseline['efficiency'] - 1) * 100
        
        print(f"  üìà Overall Improvement:")
        print(f"     Accuracy: {overall_acc_improvement:+.1f}%")
        print(f"     Token Efficiency: {overall_token_reduction:+.1f}%") 
        print(f"     Overall Efficiency: {overall_eff_improvement:+.1f}%")
        
        # Í≤∞Í≥º Ï†ÄÏû•
        timestamp = int(time.time())
        output_file = Path(__file__).parent.parent / "results" / f"adaptive_system_results_{timestamp}.json"
        output_file.parent.mkdir(exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "experiment": "adaptive_system_test",
                "timestamp": timestamp,
                "results": results,
                "summary": {
                    "overall_baseline": overall_baseline,
                    "overall_adaptive": overall_adaptive,
                    "improvement_rates": {
                        "accuracy": overall_acc_improvement,
                        "token_efficiency": overall_token_reduction,
                        "overall_efficiency": overall_eff_improvement
                    }
                }
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\\nResults saved: {output_file}")
        
        # ÏµúÏ¢Ö Í≤∞Î°†
        print(f"\\nüéØ Conclusion:")
        if overall_eff_improvement > 10:
            print(f"  ‚úÖ Adaptive system shows {overall_eff_improvement:.1f}% efficiency improvement!")
            print(f"  üéØ Smart model selection is significantly better than one-size-fits-all")
        elif overall_eff_improvement > 0:
            print(f"  ‚úÖ Adaptive system shows modest {overall_eff_improvement:.1f}% improvement")
            print(f"  üí° Targeted optimization provides measurable benefits")
        else:
            print(f"  ü§î Adaptive system shows {overall_eff_improvement:.1f}% change")
            print(f"  üìö More analysis needed to understand the patterns")

if __name__ == "__main__":
    main()