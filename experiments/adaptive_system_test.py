#!/usr/bin/env python3
"""
Project ArkhÄ“ - Adaptive System Test
ìƒˆë¡œìš´ ë¬¸ì œë“¤ì— ëŒ€í•œ ì§€ëŠ¥í˜• ëª¨ë¸ ë°°ì¹˜ í…ŒìŠ¤íŠ¸
"""

import json
import time
import sys
import os
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from llm.simple_llm import create_llm_auto
from orchestrator.adaptive_system import AdaptiveOrchestrator
from utils.scorers import score_task

def load_adaptive_test_problems():
    """ìƒˆë¡œìš´ í…ŒìŠ¤íŠ¸ ë¬¸ì œë“¤ ë¡œë“œ"""
    tasks_file = Path(__file__).parent.parent / "prompts" / "adaptive_test_problems.jsonl"
    tasks = []
    
    with open(tasks_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                tasks.append(json.loads(line))
    
    return tasks

def test_baseline_approach(task, llm_func):
    """ê¸°ì¡´ ë°©ì‹: í•­ìƒ llama3:8b Single ì‚¬ìš©"""
    start_time = time.time()
    
    llm = llm_func("llama3:8b")
    response_dict = llm.generate(task["prompt"])
    response = response_dict.get('response', str(response_dict)) if isinstance(response_dict, dict) else str(response_dict)
    
    end_time = time.time()
    
    # í† í° ê³„ì‚° (ì¶”ì •)
    tokens = len(task["prompt"].split()) + len(response.split())
    
    return {
        "approach": "baseline",
        "model_used": "llama3:8b",
        "response": response,
        "time": end_time - start_time,
        "tokens": tokens,
        "decision_rationale": "Always use powerful single model"
    }

def test_adaptive_approach(task, llm_func, orchestrator):
    """ìƒˆë¡œìš´ ë°©ì‹: ì ì‘í˜• ëª¨ë¸ ì„ íƒ"""
    start_time = time.time()
    
    result = orchestrator.execute_optimal_solution(task["prompt"], llm_func)
    
    end_time = time.time()
    result["time"] = end_time - start_time
    result["approach"] = "adaptive"
    
    return result

def calculate_metrics(result, task):
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°"""
    # ì •í™•ë„ í‰ê°€
    score = score_task(task.get("type", "reason"), task["answer"], result["response"])
    accuracy = score["score"]
    
    # íš¨ìœ¨ì„± ê³„ì‚°
    tokens = result.get("tokens", 0)
    efficiency = accuracy / (tokens / 100) if tokens > 0 else 0
    
    return {
        "accuracy": accuracy,
        "tokens": tokens,
        "time": result.get("time", 0),
        "efficiency": efficiency,
        "reasoning": score.get("reasoning", "")
    }

def main():
    """ë©”ì¸ ì‹¤í—˜ ì‹¤í–‰"""
    print("Adaptive System Test: Smart Model Selection")
    print("=" * 60)
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    orchestrator = AdaptiveOrchestrator()
    llm_func = create_llm_auto
    
    # í…ŒìŠ¤íŠ¸ ë¬¸ì œ ë¡œë“œ
    tasks = load_adaptive_test_problems()
    print(f"Loaded test problems: {len(tasks)}")
    
    results = []
    
    # ê° ë¬¸ì œì— ëŒ€í•´ í…ŒìŠ¤íŠ¸
    for i, task in enumerate(tasks):
        print(f"\n--- Test {i+1}: {task['id']} ---")
        print(f"Expected: {task['expected_config']}, Complexity: {task['expected_complexity']}")
        print(f"Problem: {task['prompt'][:80]}...")
        
        try:
            # Baseline í…ŒìŠ¤íŠ¸ (í•­ìƒ llama3:8b)
            print("\\n  [BASELINE] Testing with llama3:8b...")
            baseline_result = test_baseline_approach(task, llm_func)
            baseline_metrics = calculate_metrics(baseline_result, task)
            
            print("\\n  [ADAPTIVE] Testing with smart selection...")
            # Adaptive í…ŒìŠ¤íŠ¸ (ì§€ëŠ¥í˜• ì„ íƒ)
            adaptive_result = test_adaptive_approach(task, llm_func, orchestrator)
            adaptive_metrics = calculate_metrics(adaptive_result, task)
            
            # ê²°ê³¼ ë¹„êµ
            result = {
                "task": task,
                "baseline": {**baseline_result, **baseline_metrics},
                "adaptive": {**adaptive_result, **adaptive_metrics}
            }
            results.append(result)
            
            # ì¤‘ê°„ ê²°ê³¼ ì¶œë ¥
            print(f"\\n  Results:")
            print(f"    Baseline:  accuracy {baseline_metrics['accuracy']:.2f}, tokens {baseline_metrics['tokens']}, time {baseline_metrics['time']:.1f}s")
            print(f"    Adaptive:  accuracy {adaptive_metrics['accuracy']:.2f}, tokens {adaptive_metrics['tokens']}, time {adaptive_metrics['time']:.1f}s")
            
            # íš¨ìœ¨ì„± ë¹„êµ
            efficiency_gain = (adaptive_metrics['efficiency'] / baseline_metrics['efficiency'] - 1) * 100 if baseline_metrics['efficiency'] > 0 else 0
            accuracy_change = (adaptive_metrics['accuracy'] - baseline_metrics['accuracy']) * 100
            
            print(f"    Adaptive vs Baseline:")
            print(f"       Accuracy: {accuracy_change:+.1f}%")  
            print(f"       Efficiency: {efficiency_gain:+.1f}%")
            print(f"       Decision: {adaptive_result.get('configuration', {}).get('rationale', 'N/A')}")
            
        except Exception as e:
            print(f"    Error: {e}")
            continue
    
    # ì „ì²´ ê²°ê³¼ ë¶„ì„
    print("\\n" + "=" * 60)
    print("Overall Performance Analysis")
    print("=" * 60)
    
    if results:
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
        categories = {}
        for result in results:
            task_type = result["task"].get("type", "unknown")
            if task_type not in categories:
                categories[task_type] = {"baseline": [], "adaptive": []}
            
            categories[task_type]["baseline"].append(result["baseline"])
            categories[task_type]["adaptive"].append(result["adaptive"])
        
        print("\\nðŸ“Š Performance by Category:")
        
        overall_baseline = {"accuracy": 0, "tokens": 0, "time": 0, "efficiency": 0}
        overall_adaptive = {"accuracy": 0, "tokens": 0, "time": 0, "efficiency": 0}
        
        for category, data in categories.items():
            print(f"\\nðŸ·ï¸  {category.upper()}:")
            
            # í‰ê·  ê³„ì‚°
            baseline_avg = {
                "accuracy": sum(r["accuracy"] for r in data["baseline"]) / len(data["baseline"]),
                "tokens": sum(r["tokens"] for r in data["baseline"]) / len(data["baseline"]), 
                "time": sum(r["time"] for r in data["baseline"]) / len(data["baseline"]),
                "efficiency": sum(r["efficiency"] for r in data["baseline"]) / len(data["baseline"])
            }
            
            adaptive_avg = {
                "accuracy": sum(r["accuracy"] for r in data["adaptive"]) / len(data["adaptive"]),
                "tokens": sum(r["tokens"] for r in data["adaptive"]) / len(data["adaptive"]),
                "time": sum(r["time"] for r in data["adaptive"]) / len(data["adaptive"]), 
                "efficiency": sum(r["efficiency"] for r in data["adaptive"]) / len(data["adaptive"])
            }
            
            print(f"    Baseline:  acc {baseline_avg['accuracy']:.2f}, tok {baseline_avg['tokens']:.0f}, eff {baseline_avg['efficiency']:.3f}")
            print(f"    Adaptive:  acc {adaptive_avg['accuracy']:.2f}, tok {adaptive_avg['tokens']:.0f}, eff {adaptive_avg['efficiency']:.3f}")
            
            # ê°œì„ ìœ¨ ê³„ì‚°
            acc_improvement = (adaptive_avg['accuracy'] - baseline_avg['accuracy']) * 100
            token_reduction = (1 - adaptive_avg['tokens'] / baseline_avg['tokens']) * 100 if baseline_avg['tokens'] > 0 else 0
            eff_improvement = (adaptive_avg['efficiency'] / baseline_avg['efficiency'] - 1) * 100 if baseline_avg['efficiency'] > 0 else 0
            
            print(f"    ðŸ“ˆ Improvement: acc {acc_improvement:+.1f}%, tok {token_reduction:+.1f}%, eff {eff_improvement:+.1f}%")
            
            # ì „ì²´ í‰ê· ì— ì¶”ê°€
            for key in overall_baseline:
                overall_baseline[key] += baseline_avg[key] * len(data["baseline"])
                overall_adaptive[key] += adaptive_avg[key] * len(data["adaptive"])
        
        # ì „ì²´ í‰ê·  ê³„ì‚°
        total_count = len(results)
        for key in overall_baseline:
            overall_baseline[key] /= total_count
            overall_adaptive[key] /= total_count
        
        print(f"\\nðŸŽ¯ OVERALL PERFORMANCE:")
        print(f"  Baseline:  acc {overall_baseline['accuracy']:.2f}, tok {overall_baseline['tokens']:.0f}, eff {overall_baseline['efficiency']:.3f}")
        print(f"  Adaptive:  acc {overall_adaptive['accuracy']:.2f}, tok {overall_adaptive['tokens']:.0f}, eff {overall_adaptive['efficiency']:.3f}")
        
        # ì „ì²´ ê°œì„ ìœ¨
        overall_acc_improvement = (overall_adaptive['accuracy'] - overall_baseline['accuracy']) * 100
        overall_token_reduction = (1 - overall_adaptive['tokens'] / overall_baseline['tokens']) * 100
        overall_eff_improvement = (overall_adaptive['efficiency'] / overall_baseline['efficiency'] - 1) * 100
        
        print(f"  ðŸ“ˆ Overall Improvement:")
        print(f"     Accuracy: {overall_acc_improvement:+.1f}%")
        print(f"     Token Efficiency: {overall_token_reduction:+.1f}%") 
        print(f"     Overall Efficiency: {overall_eff_improvement:+.1f}%")
        
        # ê²°ê³¼ ì €ìž¥
        timestamp = int(time.time())
        output_file = Path(__file__).parent.parent / "results" / f"adaptive_system_results_{timestamp}.json"
        output_file.parent.mkdir(exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "experiment": "adaptive_system_test",
                "timestamp": timestamp,
                "results": results,
                "summary": {
                    "overall_baseline": overall_baseline,
                    "overall_adaptive": overall_adaptive,
                    "improvement_rates": {
                        "accuracy": overall_acc_improvement,
                        "token_efficiency": overall_token_reduction,
                        "overall_efficiency": overall_eff_improvement
                    }
                }
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\\nResults saved: {output_file}")
        
        # ìµœì¢… ê²°ë¡ 
        print(f"\\nðŸŽ¯ Conclusion:")
        if overall_eff_improvement > 10:
            print(f"  âœ… Adaptive system shows {overall_eff_improvement:.1f}% efficiency improvement!")
            print(f"  ðŸŽ¯ Smart model selection is significantly better than one-size-fits-all")
        elif overall_eff_improvement > 0:
            print(f"  âœ… Adaptive system shows modest {overall_eff_improvement:.1f}% improvement")
            print(f"  ðŸ’¡ Targeted optimization provides measurable benefits")
        else:
            print(f"  ðŸ¤” Adaptive system shows {overall_eff_improvement:.1f}% change")
            print(f"  ðŸ“š More analysis needed to understand the patterns")

if __name__ == "__main__":
    main()