#!/usr/bin/env python3
"""
Project ArkhÄ“ - Creative Collaboration Test
Multi-Agentê°€ ì •ë§ ë¹›ë‚  ìˆ˜ ìžˆëŠ” ì°½ì˜ì  í˜‘ì—… ì˜ì—­ í…ŒìŠ¤íŠ¸
"""

import json
import time
import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from llm.simple_llm import create_llm_auto
from orchestrator.pipeline import run_3stage_with_context

def run_creative_brainstorming(topic, llm_factory):
    """ì°½ì˜ì  ë¸Œë ˆì¸ìŠ¤í† ë°: ê° Agentê°€ ë‹¤ë¥¸ ê´€ì ì—ì„œ ì•„ì´ë””ì–´ ìƒì„±"""
    
    # ê° Agentì—ê²Œ ë‹¤ë¥¸ íŽ˜ë¥´ì†Œë‚˜ì™€ ê´€ì  ë¶€ì—¬
    personas = {
        "optimist": "ë‹¹ì‹ ì€ ë‚™ê´€ì ì´ê³  ê°€ëŠ¥ì„±ì„ ë³´ëŠ” ë¯¸ëž˜í•™ìžìž…ë‹ˆë‹¤. ê¸ì •ì ì´ê³  í˜ì‹ ì ì¸ ê´€ì ì—ì„œ",
        "realist": "ë‹¹ì‹ ì€ í˜„ì‹¤ì ì´ê³  ì‹¤ìš©ì ì¸ ì—”ì§€ë‹ˆì–´ìž…ë‹ˆë‹¤. êµ¬í˜„ ê°€ëŠ¥ì„±ê³¼ ì œì•½ì‚¬í•­ì„ ê³ ë ¤í•˜ì—¬", 
        "contrarian": "ë‹¹ì‹ ì€ ë¹„íŒì  ì‚¬ê³ ë¥¼ í•˜ëŠ” ì² í•™ìžìž…ë‹ˆë‹¤. ë¬¸ì œì ê³¼ ì˜ˆìƒì¹˜ ëª»í•œ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬"
    }
    
    prompt = f"""
{topic}ì— ëŒ€í•œ í˜ì‹ ì ì¸ ì•„ì´ë””ì–´ë¥¼ 3ê°€ì§€ ì œì‹œí•˜ì‹œì˜¤.

ê° ë‹¨ê³„ë³„ ê´€ì :
1. Draft: {personas["optimist"]} ìžìœ ë¡­ê³  ì°½ì˜ì ì¸ ì•„ì´ë””ì–´ë¥¼ ì œì‹œ
2. Review: {personas["realist"]} Draftì˜ ì•„ì´ë””ì–´ë¥¼ í˜„ì‹¤ì„± ìžˆê²Œ ê°œì„ 
3. Judge: {personas["contrarian"]} ëª¨ë“  ì•„ì´ë””ì–´ì˜ ìž¥ë‹¨ì ì„ ì¢…í•© í‰ê°€

ì„œë¡œ ë‹¤ë¥¸ ê´€ì ì´ ë§Œë‚˜ ë” í’ë¶€í•œ ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ë³´ì„¸ìš”.
"""
    
    start_time = time.time()
    result = run_3stage_with_context(llm_factory, prompt)
    end_time = time.time()
    
    return {
        "topic": topic,
        "approach": "creative_multi_agent", 
        "result": result,
        "time": end_time - start_time,
        "stages": 3
    }

def run_single_creative(topic, llm_factory):
    """ë‹¨ì¼ ëª¨ë¸ë¡œ ì°½ì˜ì  ìž‘ì—… ìˆ˜í–‰"""
    
    prompt = f"{topic}ì— ëŒ€í•œ í˜ì‹ ì ì¸ ì•„ì´ë””ì–´ë¥¼ 3ê°€ì§€ ì œì‹œí•˜ì‹œì˜¤."
    
    start_time = time.time()
    llm = llm_factory("llama3:8b")
    response_dict = llm.generate(prompt)
    response = response_dict.get('response', str(response_dict)) if isinstance(response_dict, dict) else str(response_dict)
    end_time = time.time()
    
    return {
        "topic": topic,
        "approach": "single_model",
        "result": {"final": response},
        "time": end_time - start_time,
        "stages": 1
    }

def analyze_creativity_metrics(multi_result, single_result):
    """ì°½ì˜ì„± ë©”íŠ¸ë¦­ ë¶„ì„"""
    
    # ì•„ì´ë””ì–´ ë‹¤ì–‘ì„± ì¸¡ì • (ë‹¨ì–´ ë‹¤ì–‘ì„±ìœ¼ë¡œ ê·¼ì‚¬)
    multi_text = str(multi_result["result"])
    single_text = str(single_result["result"])
    
    multi_words = set(multi_text.lower().split())
    single_words = set(single_text.lower().split())
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    metrics = {
        "multi_word_diversity": len(multi_words),
        "single_word_diversity": len(single_words),
        "diversity_ratio": len(multi_words) / len(single_words) if len(single_words) > 0 else 0,
        
        "multi_length": len(multi_text),
        "single_length": len(single_text),
        "elaboration_ratio": len(multi_text) / len(single_text) if len(single_text) > 0 else 0,
        
        "multi_time": multi_result["time"],
        "single_time": single_result["time"],
        "efficiency_cost": multi_result["time"] / single_result["time"] if single_result["time"] > 0 else 0
    }
    
    return metrics

def main():
    """ì°½ì˜ì  í˜‘ì—… ì‹¤í—˜ ë©”ì¸"""
    print("Creative Collaboration Test: Where Multi-Agent Shines")
    print("=" * 60)
    
    # ì°½ì˜ì  ë¬¸ì œë“¤ (ì •ë‹µì´ ì—†ê³  ë‹¤ì–‘ì„±ì´ ì¤‘ìš”í•œ ì˜ì—­)
    creative_topics = [
        "ë¯¸ëž˜ ë„ì‹œì˜ êµí†µ ì²´ì¦ í•´ê²° ë°©ì•ˆ",
        "AIì™€ ì¸ê°„ì´ ê³µì¡´í•˜ëŠ” ìƒˆë¡œìš´ ì§ì—…êµ°",
        "ê¸°í›„ë³€í™”ì— ëŒ€ì‘í•˜ëŠ” í˜ì‹ ì  ë¼ì´í”„ìŠ¤íƒ€ì¼",
        "ë©”íƒ€ë²„ìŠ¤ì—ì„œì˜ ìƒˆë¡œìš´ êµìœ¡ ë°©ì‹",
        "ìš°ì£¼ ì‹œëŒ€ì˜ ìƒˆë¡œìš´ ìŠ¤í¬ì¸  ì¢…ëª©"
    ]
    
    llm_factory = create_llm_auto
    results = []
    
    for i, topic in enumerate(creative_topics[:3]):  # 3ê°œë§Œ í…ŒìŠ¤íŠ¸
        print(f"\n--- Creative Test {i+1}: {topic} ---")
        
        try:
            # Multi-Agent ì°½ì˜ì  í˜‘ì—…
            print("  Testing Multi-Agent Creative Collaboration...")
            multi_result = run_creative_brainstorming(topic, llm_factory)
            
            # Single Model ì°½ì˜ì  ìž‘ì—…
            print("  Testing Single Model Creative Work...")
            single_result = run_single_creative(topic, llm_factory)
            
            # ì°½ì˜ì„± ë©”íŠ¸ë¦­ ë¶„ì„
            metrics = analyze_creativity_metrics(multi_result, single_result)
            
            result = {
                "topic": topic,
                "multi_agent": multi_result,
                "single_model": single_result,
                "creativity_metrics": metrics
            }
            results.append(result)
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"\n  Results:")
            print(f"    Word Diversity: Multi {metrics['multi_word_diversity']} vs Single {metrics['single_word_diversity']} (ratio: {metrics['diversity_ratio']:.2f})")
            print(f"    Elaboration: Multi {metrics['multi_length']} vs Single {metrics['single_length']} chars (ratio: {metrics['elaboration_ratio']:.2f})")
            print(f"    Time Cost: {metrics['efficiency_cost']:.2f}x slower")
            
            # ì°½ì˜ì„± íŒë‹¨
            if metrics['diversity_ratio'] > 1.2 and metrics['elaboration_ratio'] > 1.1:
                print(f"    ðŸŒŸ Multi-Agent shows superior creativity!")
            elif metrics['diversity_ratio'] > 1.0:
                print(f"    âœ¨ Multi-Agent shows better diversity")
            else:
                print(f"    ðŸ¤” Single Model performs comparably")
                
        except Exception as e:
            print(f"    Error: {e}")
            continue
    
    # ì „ì²´ ë¶„ì„
    print("\n" + "=" * 60)
    print("Creative Collaboration Analysis")
    print("=" * 60)
    
    if results:
        # í‰ê·  ë©”íŠ¸ë¦­
        avg_metrics = {
            "diversity_ratio": sum(r["creativity_metrics"]["diversity_ratio"] for r in results) / len(results),
            "elaboration_ratio": sum(r["creativity_metrics"]["elaboration_ratio"] for r in results) / len(results),
            "efficiency_cost": sum(r["creativity_metrics"]["efficiency_cost"] for r in results) / len(results)
        }
        
        print(f"\nOverall Creative Performance:")
        print(f"  Average Diversity Ratio: {avg_metrics['diversity_ratio']:.2f}")
        print(f"  Average Elaboration Ratio: {avg_metrics['elaboration_ratio']:.2f}") 
        print(f"  Average Time Cost: {avg_metrics['efficiency_cost']:.2f}x")
        
        # ê²°ë¡ 
        print(f"\nCreative Collaboration Conclusion:")
        if avg_metrics['diversity_ratio'] > 1.3:
            print(f"  ðŸŽ¨ Multi-Agent excels in creative domains!")
            print(f"  ðŸ“ˆ {((avg_metrics['diversity_ratio']-1)*100):.1f}% more diverse ideas")
            print(f"  ðŸ” This is where collaborative intelligence truly shines")
        elif avg_metrics['diversity_ratio'] > 1.1:
            print(f"  ðŸŒ± Multi-Agent shows creative advantages")
            print(f"  ðŸ’¡ Worth the extra cost for creative projects")
        else:
            print(f"  ðŸ“Š Mixed results - need deeper analysis")
            
        # ê²°ê³¼ ì €ìž¥
        timestamp = int(time.time())
        output_file = Path(__file__).parent.parent / "results" / f"creative_collaboration_results_{timestamp}.json"
        output_file.parent.mkdir(exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "experiment": "creative_collaboration_test",
                "timestamp": timestamp,
                "results": results,
                "summary": avg_metrics
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nResults saved: {output_file}")

if __name__ == "__main__":
    main()