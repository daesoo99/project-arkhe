# -*- coding: utf-8 -*-
"""
ê°œì„ ëœ Multi-Agent í…ŒìŠ¤íŠ¸ - "êµìˆ˜ë‹˜" ê¶Œìœ„ êµ¬ì¡°
Draft(í•™ë¶€ì—°êµ¬ìƒ) -> Review(ì„ë°•ì‚¬) -> Judge(êµìˆ˜ë‹˜)
"""

import sys
import time
import json
from typing import List, Dict, Any
from dataclasses import dataclass
sys.path.append('.')

from src.llm.simple_llm import create_llm_auto

@dataclass 
class ImprovedResult:
    """ê°œì„ ëœ ì‹¤í—˜ ê²°ê³¼"""
    method: str
    question: str
    expected: str
    predicted: str
    correct: bool
    tokens: int
    time_ms: int
    draft_responses: List[str] = None
    review_responses: List[str] = None
    judge_reasoning: str = ""

class ImprovedMultiAgentTester:
    """ê°œì„ ëœ Multi-Agent í…ŒìŠ¤í„°"""
    
    def __init__(self):
        print("ğŸ“ í•™ê³„ ëª¨ë¸ êµ¬ì¡° ë¡œë”©...")
        self.undergraduate = create_llm_auto("qwen2:0.5b")  # í•™ë¶€ì—°êµ¬ìƒ
        self.graduate = create_llm_auto("qwen2:7b")         # ì„ë°•ì‚¬  
        self.professor = create_llm_auto("llama3:8b")       # êµìˆ˜ë‹˜
        print("âœ… í•™ê³„ êµ¬ì¡° ì¤€ë¹„ ì™„ë£Œ")
    
    def run_original_multiagent(self, question: str, expected: str) -> ImprovedResult:
        """ê¸°ì¡´ Multi-Agent (íœ˜ë‘˜ë¦¬ëŠ” Judge)"""
        start_time = time.time()
        total_tokens = 0
        
        # Draft stage (í•™ë¶€ì—°êµ¬ìƒë“¤)
        draft_responses = []
        for i in range(3):
            prompt = f"Question: {question}\n\nSolve step by step:"
            response = self.undergraduate.generate(prompt, temperature=0.2 + i*0.1, max_tokens=150)
            
            if isinstance(response, dict):
                draft = response.get("response", "").strip()
            else:
                draft = str(response).strip()
            
            draft_responses.append(draft)
            total_tokens += len(prompt.split()) + len(draft.split())
        
        # Review stage (ì„ë°•ì‚¬ë“¤)
        review_responses = []
        for reviewer_id in range(2):
            review_prompt = f"""Question: {question}

Undergraduate students' attempts:
{chr(10).join(f"Student {i+1}: {resp}" for i, resp in enumerate(draft_responses))}

As graduate student reviewer {reviewer_id + 1}:
1. Check mathematical accuracy
2. Identify good approaches and errors
3. Provide analysis

Review:"""

            response = self.graduate.generate(review_prompt, temperature=0.3, max_tokens=200)
            if isinstance(response, dict):
                review = response.get("response", "").strip()
            else:
                review = str(response).strip()
            
            review_responses.append(review)
            total_tokens += len(review_prompt.split()) + len(review.split())
        
        # Judge stage (ê¸°ì¡´ ìŠ¤íƒ€ì¼ - íœ˜ë‘˜ë¦¬ëŠ” êµìˆ˜ë‹˜)
        judge_prompt = f"""Question: {question}

Student attempts:
{chr(10).join(f"Student {i+1}: {draft}" for i, draft in enumerate(draft_responses))}

Graduate reviews:
Review 1: {review_responses[0]}
Review 2: {review_responses[1]}

As judge:
1. Analyze all the provided information
2. Consider the student attempts and graduate reviews
3. Synthesize a final answer based on the collective input

Final answer:"""

        response = self.professor.generate(judge_prompt, temperature=0.1, max_tokens=150)
        if isinstance(response, dict):
            judge_answer = response.get("response", "").strip()
        else:
            judge_answer = str(response).strip()
        
        total_tokens += len(judge_prompt.split()) + len(judge_answer.split())
        time_ms = int((time.time() - start_time) * 1000)
        
        predicted = self.extract_answer(judge_answer)
        correct = self.is_correct(predicted, expected)
        
        return ImprovedResult(
            method="Original Multi-Agent (Collaborative Judge)",
            question=question,
            expected=expected,
            predicted=predicted,
            correct=correct,
            tokens=total_tokens,
            time_ms=time_ms,
            draft_responses=draft_responses,
            review_responses=review_responses,
            judge_reasoning=judge_answer
        )
    
    def run_improved_multiagent(self, question: str, expected: str) -> ImprovedResult:
        """ê°œì„ ëœ Multi-Agent (êµìˆ˜ë‹˜ ê¶Œìœ„)"""
        start_time = time.time()
        total_tokens = 0
        
        # Draft stage (ë™ì¼)
        draft_responses = []
        for i in range(3):
            prompt = f"Question: {question}\n\nAs undergraduate researcher, solve this step by step:"
            response = self.undergraduate.generate(prompt, temperature=0.2 + i*0.1, max_tokens=150)
            
            if isinstance(response, dict):
                draft = response.get("response", "").strip()
            else:
                draft = str(response).strip()
            
            draft_responses.append(draft)
            total_tokens += len(prompt.split()) + len(draft.split())
        
        # Review stage (ë™ì¼)
        review_responses = []
        for reviewer_id in range(2):
            review_prompt = f"""Question: {question}

Undergraduate attempts:
{chr(10).join(f"Student {i+1}: {resp}" for i, resp in enumerate(draft_responses))}

As graduate student, provide critical analysis:
1. Which approaches show promise?
2. What errors do you spot?
3. What improvements would you suggest?

Critical review:"""

            response = self.graduate.generate(review_prompt, temperature=0.3, max_tokens=200)
            if isinstance(response, dict):
                review = response.get("response", "").strip()
            else:
                review = str(response).strip()
            
            review_responses.append(review)
            total_tokens += len(review_prompt.split()) + len(review.split())
        
        # Judge stage (ìƒˆë¡œìš´ ìŠ¤íƒ€ì¼ - ê¶Œìœ„ì  êµìˆ˜ë‹˜)
        judge_prompt = f"""I am a professor and expert in this field.

PROBLEM: {question}

Undergraduate students' attempts:
{chr(10).join(f"Student {i+1}: {draft}" for i, draft in enumerate(draft_responses))}

Graduate students' reviews:
Review 1: {review_responses[0]}
Review 2: {review_responses[1]}

PROFESSOR'S INDEPENDENT EXPERT JUDGMENT:

First, let me solve this problem using my expertise:
1. I will approach this problem with my professional knowledge
2. I will examine if any undergraduate ideas are worth incorporating
3. I will verify if graduate reviews are sound
4. I will make the final decision based on my expertise

Key principles:
- I reference others' work but make independent judgments
- If students or graduates are wrong, I correct them without hesitation
- I adopt good ideas and discard bad ones
- My expertise takes precedence in the final decision

Professional analysis and final answer:"""

        response = self.professor.generate(judge_prompt, temperature=0.1, max_tokens=200)
        if isinstance(response, dict):
            judge_answer = response.get("response", "").strip()
        else:
            judge_answer = str(response).strip()
        
        total_tokens += len(judge_prompt.split()) + len(judge_answer.split())
        time_ms = int((time.time() - start_time) * 1000)
        
        predicted = self.extract_answer(judge_answer)
        correct = self.is_correct(predicted, expected)
        
        return ImprovedResult(
            method="Improved Multi-Agent (Authoritative Professor)",
            question=question,
            expected=expected,
            predicted=predicted,
            correct=correct,
            tokens=total_tokens,
            time_ms=time_ms,
            draft_responses=draft_responses,
            review_responses=review_responses,
            judge_reasoning=judge_answer
        )
    
    def run_single_professor(self, question: str, expected: str) -> ImprovedResult:
        """Single êµìˆ˜ë‹˜ ëª¨ë¸"""
        start_time = time.time()
        
        prompt = f"As an expert professor, solve this problem: {question}\n\nProvide the answer:"
        response = self.professor.generate(prompt, temperature=0.1, max_tokens=150)
        
        if isinstance(response, dict):
            answer = response.get("response", "").strip()
        else:
            answer = str(response).strip()
        
        tokens = len(prompt.split()) + len(answer.split())
        time_ms = int((time.time() - start_time) * 1000)
        
        predicted = self.extract_answer(answer)
        correct = self.is_correct(predicted, expected)
        
        return ImprovedResult(
            method="Single Professor Model",
            question=question,
            expected=expected,
            predicted=predicted,
            correct=correct,
            tokens=tokens,
            time_ms=time_ms,
            judge_reasoning=answer
        )
    
    def extract_answer(self, text: str) -> str:
        """ë‹µë³€ì—ì„œ ìˆ«ì ì¶”ì¶œ"""
        import re
        
        patterns = [
            r'(?:answer is|answer:|final answer is|final answer:)\s*([+-]?\d+(?:\.\d+)?)',
            r'(?:result is|result:)\s*([+-]?\d+(?:\.\d+)?)',
            r'=\s*([+-]?\d+(?:\.\d+)?)',
            r'([+-]?\d+(?:\.\d+)?)\s*$',
            r'([+-]?\d+(?:\.\d+)?)\s*\.',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text.lower())
            if match:
                return match.group(1)
        
        numbers = re.findall(r'([+-]?\d+(?:\.\d+)?)', text)
        if numbers:
            return numbers[-1]
        
        return ""
    
    def is_correct(self, predicted: str, expected: str) -> bool:
        """ì •ë‹µ ì—¬ë¶€ í™•ì¸"""
        if not predicted:
            return False
        
        try:
            pred_num = float(predicted)
            exp_num = float(expected)
            return abs(pred_num - exp_num) < 0.01
        except:
            return predicted.strip().lower() == expected.strip().lower()

def run_authority_comparison_test():
    """ê¶Œìœ„ êµ¬ì¡° ë¹„êµ í…ŒìŠ¤íŠ¸"""
    
    print("=" * 80)
    print("ğŸ“ êµìˆ˜ë‹˜ ê¶Œìœ„ êµ¬ì¡° ë¹„êµ í…ŒìŠ¤íŠ¸")
    print("Original vs Improved Multi-Agent vs Single Professor")
    print("=" * 80)
    
    tester = ImprovedMultiAgentTester()
    
    # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ - ì‹¤ì œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì‹¤íŒ¨í–ˆë˜ ë¬¸ì œë“¤
    test_cases = [
        {"question": "Sarah has 15 apples. She gives away 7. How many are left?", "expected": "8"},
        {"question": "A shirt costs $25. With 20% discount, what is the final price?", "expected": "20"},
        {"question": "If 4 friends share 36 chocolates equally, how many does each get?", "expected": "9"},
        {"question": "What is 240 divided by 140?", "expected": "1.714"},
        {"question": "Rectangle is 12m by 8m. What is the perimeter?", "expected": "40"},
        {"question": "Two trains 240 miles apart, speeds 60 and 80 mph. Meeting time?", "expected": "1.714"},
    ]
    
    all_results = []
    
    for i, test_case in enumerate(test_cases):
        question = test_case["question"]
        expected = test_case["expected"]
        
        print(f"\nğŸ“ Problem {i+1}: {question}")
        print("-" * 60)
        
        # Original Multi-Agent
        print("ğŸ¤ Original Multi-Agent (Collaborative)...")
        original_result = tester.run_original_multiagent(question, expected)
        
        # Improved Multi-Agent  
        print("ğŸ‘¨â€ğŸ« Improved Multi-Agent (Authoritative)...")
        improved_result = tester.run_improved_multiagent(question, expected)
        
        # Single Professor
        print("ğŸ¯ Single Professor...")
        single_result = tester.run_single_professor(question, expected)
        
        # ê²°ê³¼ ì¶œë ¥
        results = [original_result, improved_result, single_result]
        print(f"\nğŸ“Š Results:")
        
        for result in results:
            status = "âœ…" if result.correct else "âŒ"
            print(f"  {status} {result.method}")
            print(f"      Predicted: {result.predicted} (Expected: {result.expected})")
            print(f"      Tokens: {result.tokens}, Time: {result.time_ms}ms")
            
            # Judgeì˜ reasoning ìƒ˜í”Œ
            if "Multi-Agent" in result.method and result.judge_reasoning:
                reasoning_sample = result.judge_reasoning[:100].replace('\n', ' ')
                print(f"      Judge reasoning: {reasoning_sample}...")
        
        all_results.extend(results)
    
    # ì¢…í•© ë¶„ì„
    analyze_authority_results(all_results)
    save_authority_results(all_results)
    
    return all_results

def analyze_authority_results(results: List[ImprovedResult]):
    """ê¶Œìœ„ êµ¬ì¡° ê²°ê³¼ ë¶„ì„"""
    
    print(f"\n" + "=" * 80)
    print("ğŸ“Š ê¶Œìœ„ êµ¬ì¡° ë¹„êµ ë¶„ì„")
    print("=" * 80)
    
    # ë°©ë²•ë³„ ê·¸ë£¹í™”
    methods = {}
    for result in results:
        if result.method not in methods:
            methods[result.method] = []
        methods[result.method].append(result)
    
    print(f"\nğŸ¯ ì „ì²´ ì„±ëŠ¥ ë¹„êµ:")
    for method, method_results in methods.items():
        correct_count = sum(1 for r in method_results if r.correct)
        total_count = len(method_results)
        accuracy = correct_count / total_count if total_count > 0 else 0
        
        avg_tokens = sum(r.tokens for r in method_results) / total_count
        avg_time = sum(r.time_ms for r in method_results) / total_count
        
        print(f"\n{method}:")
        print(f"  ì •í™•ë„: {accuracy:.1%} ({correct_count}/{total_count})")
        print(f"  í‰ê·  í† í°: {avg_tokens:.0f}")
        print(f"  í‰ê·  ì‹œê°„: {avg_time:.0f}ms")
    
    # ê°œì„  íš¨ê³¼ ë¶„ì„
    original_results = methods.get("Original Multi-Agent (Collaborative Judge)", [])
    improved_results = methods.get("Improved Multi-Agent (Authoritative Professor)", [])
    single_results = methods.get("Single Professor Model", [])
    
    if original_results and improved_results:
        orig_acc = sum(1 for r in original_results if r.correct) / len(original_results)
        imp_acc = sum(1 for r in improved_results if r.correct) / len(improved_results)
        single_acc = sum(1 for r in single_results if r.correct) / len(single_results)
        
        print(f"\nğŸ” ê¶Œìœ„ êµ¬ì¡° ê°œì„  íš¨ê³¼:")
        print(f"  Original Multi-Agent: {orig_acc:.1%}")
        print(f"  Improved Multi-Agent: {imp_acc:.1%}")
        print(f"  Single Professor: {single_acc:.1%}")
        
        improvement = imp_acc - orig_acc
        vs_single = imp_acc - single_acc
        
        print(f"  ê°œì„  íš¨ê³¼: {improvement:+.1%}")
        if improvement > 0.05:
            print(f"  âœ… ê¶Œìœ„ì  Judgeê°€ í˜‘ë ¥ì  Judgeë³´ë‹¤ íš¨ê³¼ì !")
        
        print(f"  vs Single: {vs_single:+.1%}")
        if vs_single > 0:
            print(f"  âœ… Multi-Agentê°€ Single Modelì„ ëŠ¥ê°€!")
        elif vs_single > -0.1:
            print(f"  ğŸ“Š Single Modelê³¼ ë¹„ìŠ·í•œ ì„±ëŠ¥")
        else:
            print(f"  âŒ ì—¬ì „íˆ Single Modelì´ ìš°ì„¸")

def save_authority_results(results: List[ImprovedResult]):
    """ê¶Œìœ„ êµ¬ì¡° ì‹¤í—˜ ê²°ê³¼ ì €ì¥"""
    
    timestamp = int(time.time())
    filename = f"results/authority_comparison_{timestamp}.json"
    
    serializable_results = []
    for result in results:
        serializable_results.append({
            "method": result.method,
            "question": result.question,
            "expected": result.expected,
            "predicted": result.predicted,
            "correct": result.correct,
            "tokens": result.tokens,
            "time_ms": result.time_ms,
            "draft_responses": result.draft_responses,
            "review_responses": result.review_responses,
            "judge_reasoning": result.judge_reasoning
        })
    
    try:
        import os
        os.makedirs("results", exist_ok=True)
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ê¶Œìœ„ êµ¬ì¡° ì‹¤í—˜ ê²°ê³¼ ì €ì¥: {filename}")
        
    except Exception as e:
        print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    print("ğŸ“ êµìˆ˜ë‹˜ ê¶Œìœ„ êµ¬ì¡° í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("ğŸ“ ê¸°ì¡´ í˜‘ë ¥ì  Judge vs ê¶Œìœ„ì  êµìˆ˜ë‹˜ Judge ë¹„êµ")
    print("â±ï¸  ì˜ˆìƒ ì†Œìš” ì‹œê°„: 10-15ë¶„")
    
    input("\nPress Enter to start authority structure test...")
    
    results = run_authority_comparison_test()
    
    print(f"\nâœ… ê¶Œìœ„ êµ¬ì¡° ë¹„êµ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"ğŸ“ˆ êµìˆ˜ë‹˜ì˜ ê¶Œìœ„ê°€ Multi-Agent ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”!")