# -*- coding: utf-8 -*-
"""
Project ArkhÄ“ - íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
ë©€í‹°ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸ ë¹Œë” ë° ì‹¤í–‰ê¸°

V2 Features:
- ContextualPipeline: ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬í˜• íŒŒì´í”„ë¼ì¸
- ë‹¨ê³„ë³„ ì¶œë ¥ì„ ë‹¤ìŒ ë‹¨ê³„ ì…ë ¥ìœ¼ë¡œ ìë™ ì—°ê²°
- ì¡°ê±´ë¶€ ìŠ¹ê²© (promote_if) ì§€ì›
"""

import time
from typing import List, Dict, Any, Optional, Union, Callable
from dataclasses import dataclass, field
from enum import Enum

class AggregationStrategy(Enum):
    """ê²°ê³¼ ì§‘ê³„ ì „ëµ"""
    MAJORITY_VOTE = "majority"      # ë‹¤ìˆ˜ê²°
    FIRST_VALID = "first_valid"     # ì²« ë²ˆì§¸ ìœ íš¨í•œ ë‹µë³€
    JUDGE_MODEL = "judge"           # íŒì • ëª¨ë¸ ì‚¬ìš©
    WEIGHTED_AVERAGE = "weighted"   # ê°€ì¤‘ í‰ê· 
    CONSENSUS = "consensus"         # í•©ì˜ ë„ì¶œ

@dataclass
class PipelineStep:
    """íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ ì •ì˜"""
    model: str                      # ëª¨ë¸ ID (ì˜ˆ: "gemma:2b", "gpt-4o-mini")
    prompt: str                     # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    temperature: float = 0.2        # ì˜¨ë„ ì„¤ì •
    max_tokens: int = 512           # ìµœëŒ€ í† í° ìˆ˜
    timeout: int = 120             # íƒ€ì„ì•„ì›ƒ (ì´ˆ)
    retry_count: int = 1           # ì¬ì‹œë„ íšŸìˆ˜

@dataclass 
class StepResult:
    """ë‹¨ê³„ ì‹¤í–‰ ê²°ê³¼"""
    step_id: int
    model: str
    response: str
    latency_ms: int
    success: bool
    error: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None

class Pipeline:
    """ë©€í‹°ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, name: str, steps: List[PipelineStep], 
                 aggregation: AggregationStrategy = AggregationStrategy.MAJORITY_VOTE):
        self.name = name
        self.steps = steps
        self.aggregation = aggregation
        self.results: List[StepResult] = []
        
    def execute(self, base_prompt: str, llm_factory=None) -> Dict[str, Any]:
        """
        íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Args:
            base_prompt: ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
            llm_factory: LLM ì¸ìŠ¤í„´ìŠ¤ íŒ©í† ë¦¬ í•¨ìˆ˜
            
        Returns:
            ì‹¤í–‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not llm_factory:
            # ê¸°ë³¸ íŒ©í† ë¦¬ (Ollama ì‚¬ìš©)
            llm_factory = self._default_llm_factory
            
        start_time = time.time()
        self.results = []
        
        # ê° ë‹¨ê³„ ìˆœì°¨ ì‹¤í–‰
        for i, step in enumerate(self.steps):
            step_result = self._execute_step(i, step, base_prompt, llm_factory)
            self.results.append(step_result)
            
        # ê²°ê³¼ ì§‘ê³„
        final_result = self._aggregate_results()
        
        total_time = time.time() - start_time
        
        # StepResult ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        step_results_dict = []
        for r in self.results:
            step_dict = {
                "step_id": r.step_id,
                "model": r.model,
                "response": r.response,
                "latency_ms": r.latency_ms,
                "success": r.success,
                "error": r.error,
                "metadata": r.metadata
            }
            step_results_dict.append(step_dict)
        
        return {
            "pipeline_name": self.name,
            "final_answer": final_result["answer"],
            "confidence": final_result["confidence"],
            "step_results": step_results_dict,
            "total_latency_ms": int(total_time * 1000),
            "successful_steps": sum(1 for r in self.results if r.success),
            "total_steps": len(self.steps),
            "aggregation_method": self.aggregation.value,
            "aggregation_details": final_result.get("details", "")
        }
    
    def _execute_step(self, step_id: int, step: PipelineStep, 
                      base_prompt: str, llm_factory) -> StepResult:
        """ë‹¨ì¼ ë‹¨ê³„ ì‹¤í–‰"""
        try:
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì ìš©
            formatted_prompt = step.prompt.format(base_prompt=base_prompt)
            
            # LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ì‹¤í–‰
            llm = llm_factory(step.model)
            start_time = time.time()
            
            response = llm.generate(
                formatted_prompt,
                temperature=step.temperature,
                max_tokens=step.max_tokens,
                timeout=step.timeout
            )
            
            latency = int((time.time() - start_time) * 1000)
            
            return StepResult(
                step_id=step_id,
                model=step.model,
                response=response.get("response", ""),
                latency_ms=latency,
                success=True,
                metadata=response
            )
            
        except Exception as e:
            return StepResult(
                step_id=step_id,
                model=step.model,
                response="",
                latency_ms=0,
                success=False,
                error=str(e)
            )
    
    def _aggregate_results(self) -> Dict[str, Any]:
        """ê²°ê³¼ ì§‘ê³„"""
        successful_results = [r for r in self.results if r.success]
        
        if not successful_results:
            return {
                "answer": "[ERROR] ëª¨ë“  ë‹¨ê³„ ì‹¤íŒ¨",
                "confidence": 0.0,
                "details": "No successful steps"
            }
        
        if self.aggregation == AggregationStrategy.MAJORITY_VOTE:
            return self._majority_vote(successful_results)
        elif self.aggregation == AggregationStrategy.FIRST_VALID:
            return self._first_valid(successful_results)
        elif self.aggregation == AggregationStrategy.CONSENSUS:
            return self._consensus(successful_results)
        else:
            # ê¸°ë³¸ê°’: ì²« ë²ˆì§¸ ì„±ê³µ ê²°ê³¼
            return self._first_valid(successful_results)
    
    def _majority_vote(self, results: List[StepResult]) -> Dict[str, Any]:
        """ë‹¤ìˆ˜ê²° ì§‘ê³„"""
        responses = [r.response.strip().lower() for r in results if r.response.strip()]
        
        if not responses:
            return {"answer": "", "confidence": 0.0, "details": "No valid responses"}
        
        # ì‘ë‹µ ë¹ˆë„ ê³„ì‚°
        response_counts = {}
        for response in responses:
            # ê°„ë‹¨í•œ ì •ê·œí™” (ì²« 50ê¸€ìë¡œ ë¹„êµ)
            key = response[:50]
            response_counts[key] = response_counts.get(key, 0) + 1
        
        # ìµœë‹¤ ë“í‘œ ì‘ë‹µ
        most_common = max(response_counts.items(), key=lambda x: x[1])
        confidence = most_common[1] / len(responses)
        
        # ì›ë³¸ ì‘ë‹µ ì°¾ê¸°
        for r in results:
            if r.response.strip().lower()[:50] == most_common[0]:
                return {
                    "answer": r.response,
                    "confidence": confidence,
                    "details": f"Votes: {most_common[1]}/{len(responses)}"
                }
        
        return {"answer": most_common[0], "confidence": confidence, "details": "Majority vote"}
    
    def _first_valid(self, results: List[StepResult]) -> Dict[str, Any]:
        """ì²« ë²ˆì§¸ ìœ íš¨ ê²°ê³¼"""
        if results:
            return {
                "answer": results[0].response,
                "confidence": 1.0,
                "details": f"First valid from {results[0].model}"
            }
        return {"answer": "", "confidence": 0.0, "details": "No results"}
    
    def _consensus(self, results: List[StepResult]) -> Dict[str, Any]:
        """í•©ì˜ ê¸°ë°˜ ì§‘ê³„ (ë‹¨ìˆœ ë²„ì „)"""
        if len(results) == 1:
            return self._first_valid(results)
        
        # ëª¨ë“  ì‘ë‹µ ê²°í•©
        combined = " | ".join([r.response for r in results if r.response])
        confidence = len(results) / len(self.steps)  # ì„±ê³µë¥  ê¸°ë°˜
        
        return {
            "answer": combined,
            "confidence": confidence,
            "details": f"Consensus from {len(results)} agents"
        }
    
    def _default_llm_factory(self, model_id: str):
        """ê¸°ë³¸ LLM íŒ©í† ë¦¬ (ìë™ ê°ì§€)"""
        from src.llm.simple_llm import create_llm_auto
        return create_llm_auto(model_id)

class PipelineBuilder:
    """íŒŒì´í”„ë¼ì¸ ë¹Œë”"""
    
    @staticmethod
    def create_single_agent(model: str, name: str = None) -> Pipeline:
        """ë‹¨ì¼ ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸"""
        if not name:
            name = f"Single-{model}"
        
        steps = [PipelineStep(
            model=model,
            prompt="{base_prompt}"
        )]
        
        return Pipeline(name, steps, AggregationStrategy.FIRST_VALID)
    
    @staticmethod  
    def create_multi_independent(models: List[str], name: str = None) -> Pipeline:
        """ë…ë¦½ ë©€í‹°ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸"""
        if not name:
            name = f"Multi-{len(models)}"
        
        steps = [
            PipelineStep(model=model, prompt="{base_prompt}")
            for model in models
        ]
        
        return Pipeline(name, steps, AggregationStrategy.MAJORITY_VOTE)
    
    @staticmethod
    def create_sequential_refinement(models: List[str], name: str = None) -> Pipeline:
        """ìˆœì°¨ì  ê°œì„  íŒŒì´í”„ë¼ì¸"""
        if not name:
            name = f"Sequential-{len(models)}"
        
        steps = []
        
        # ì²« ë²ˆì§¸ ë‹¨ê³„: ì´ˆì•ˆ ì‘ì„±
        if models:
            steps.append(PipelineStep(
                model=models[0],
                prompt="{base_prompt}"
            ))
        
        # ë‚˜ë¨¸ì§€ ë‹¨ê³„: ê°œì„  ë° ì¬í‰ê°€
        for model in models[1:]:
            steps.append(PipelineStep(
                model=model,
                prompt="ë‹¤ìŒ ë‹µë³€ì„ ê²€í† í•˜ê³  ê°œì„ í•˜ì„¸ìš”:\n\nì§ˆë¬¸: {base_prompt}\n\nì´ì „ ë‹µë³€: [ì´ì „ ë‹¨ê³„ ê²°ê³¼]\n\nê°œì„ ëœ ë‹µë³€:"
            ))
        
        return Pipeline(name, steps, AggregationStrategy.FIRST_VALID)

# ë¯¸ë¦¬ ì •ì˜ëœ íŒŒì´í”„ë¼ì¸ë“¤
PRESET_PIPELINES = {
    "single_8b": PipelineBuilder.create_single_agent("llama3:8b"),
    "triple_2b": PipelineBuilder.create_multi_independent(["gemma:2b"] * 3),
    "draft_review_final": PipelineBuilder.create_sequential_refinement(["gemma:2b", "mistral:7b", "llama3:8b"])
}

# =============================================================================
# V2: Contextual Pipeline - ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬í˜• íŒŒì´í”„ë¼ì¸
# =============================================================================

@dataclass
class ContextualPipelineStep:
    """ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬í˜• íŒŒì´í”„ë¼ì¸ ë‹¨ê³„"""
    model: str                              # ëª¨ë¸ ID
    prompt_template: str                    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ("{query}", "{draft}", etc.)
    output_key: str                         # ì´ ë‹¨ê³„ ê²°ê³¼ë¥¼ ì €ì¥í•  ì»¨í…ìŠ¤íŠ¸ í‚¤
    temperature: float = 0.2                # ì˜¨ë„ ì„¤ì •
    max_tokens: int = 512                   # ìµœëŒ€ í† í° ìˆ˜
    timeout: int = 120                      # íƒ€ì„ì•„ì›ƒ (ì´ˆ)
    promote_if: Optional[Callable[[Dict], bool]] = None  # ìŠ¹ê¸‰ ì¡°ê±´ í•¨ìˆ˜
    required_keys: List[str] = field(default_factory=list)  # í•„ìš”í•œ ì»¨í…ìŠ¤íŠ¸ í‚¤ë“¤

@dataclass
class ContextualStepResult:
    """ì»¨í…ìŠ¤íŠ¸ ë‹¨ê³„ ì‹¤í–‰ ê²°ê³¼"""
    step_id: int
    model: str
    output_key: str
    response: str
    latency_ms: int
    eval_count: int = 0
    eval_duration_ns: int = 0
    success: bool = True
    skipped: bool = False
    error: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None

class ContextualPipeline:
    """ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬í˜• íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, name: str, steps: List[ContextualPipelineStep]):
        self.name = name
        self.steps = steps
        self.context = {}
        self.results = []
        
    def run(self, query: str, llm_factory=None) -> Dict[str, Any]:
        """
        ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬í˜• íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Args:
            query: ì´ˆê¸° ì¿¼ë¦¬
            llm_factory: LLM íŒ©í† ë¦¬ í•¨ìˆ˜ (ê¸°ë³¸ê°’: create_llm_auto)
            
        Returns:
            {
                "final": ìµœì¢… ë‹µë³€,
                "context": ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ë”•ì…”ë„ˆë¦¬, 
                "metrics": ì‹¤í–‰ ë©”íŠ¸ë¦­,
                "step_results": ë‹¨ê³„ë³„ ê²°ê³¼
            }
        """
        if llm_factory is None:
            from src.llm.simple_llm import create_llm_auto
            llm_factory = create_llm_auto
            
        # ì´ˆê¸° ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
        self.context = {"query": query}
        self.results = []
        
        start_time = time.time()
        executed_steps = 0
        skipped_steps = 0
        
        # ê° ë‹¨ê³„ ìˆœì°¨ ì‹¤í–‰
        for step_id, step in enumerate(self.steps):
            step_result = self._execute_contextual_step(step_id, step, llm_factory)
            self.results.append(step_result)
            
            if step_result.skipped:
                skipped_steps += 1
                print(f"  Step {step_id + 1} ({step.model}) skipped by promote_if")
                continue
                
            if step_result.success:
                executed_steps += 1
                # ì„±ê³µí•œ ê²½ìš° ì»¨í…ìŠ¤íŠ¸ì— ê²°ê³¼ ì €ì¥
                self.context[step.output_key] = step_result.response
                print(f"  Step {step_id + 1} ({step.model}) -> context['{step.output_key}']")
            else:
                print(f"  Step {step_id + 1} ({step.model}) failed: {step_result.error}")
        
        total_time = time.time() - start_time
        
        # ìµœì¢… ê²°ê³¼ ê²°ì •
        final_answer = self._determine_final_answer()
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = {
            "total_time_ms": int(total_time * 1000),
            "executed_steps": executed_steps,
            "skipped_steps": skipped_steps,
            "total_steps": len(self.steps),
            "success_rate": executed_steps / max(len(self.steps) - skipped_steps, 1),
            "total_cost": sum(r.eval_count * 0.001 for r in self.results if r.success),  # ì¶”ì • ë¹„ìš©
            "total_tokens": sum(r.eval_count for r in self.results if r.success),
            "avg_latency": sum(r.latency_ms for r in self.results if r.success) / max(executed_steps, 1)
        }
        
        return {
            "final": final_answer,
            "context": dict(self.context),  # ë³µì‚¬ë³¸ ë°˜í™˜
            "metrics": metrics,
            "step_results": [self._step_result_to_dict(r) for r in self.results]
        }
    
    def _execute_contextual_step(self, step_id: int, step: ContextualPipelineStep, 
                                llm_factory) -> ContextualStepResult:
        """ë‹¨ì¼ ì»¨í…ìŠ¤íŠ¸ ë‹¨ê³„ ì‹¤í–‰"""
        
        # ìŠ¹ê¸‰ ì¡°ê±´ ì²´í¬
        if step.promote_if and not step.promote_if(self.context):
            return ContextualStepResult(
                step_id=step_id,
                model=step.model,
                output_key=step.output_key,
                response="",
                latency_ms=0,
                success=True,
                skipped=True
            )
        
        # í•„ìš”í•œ í‚¤ ì²´í¬
        missing_keys = [k for k in step.required_keys if k not in self.context]
        if missing_keys:
            return ContextualStepResult(
                step_id=step_id,
                model=step.model,
                output_key=step.output_key,
                response="",
                latency_ms=0,
                success=False,
                error=f"Missing required keys: {missing_keys}"
            )
        
        try:
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì ìš©
            formatted_prompt = step.prompt_template.format(**self.context)
            
            # LLM ì‹¤í–‰
            llm = llm_factory(step.model)
            start_time = time.time()
            
            response = llm.generate(
                formatted_prompt,
                temperature=step.temperature,
                max_tokens=step.max_tokens
            )
            
            latency = int((time.time() - start_time) * 1000)
            
            # ì‘ë‹µ ì²˜ë¦¬
            if isinstance(response, dict):
                response_text = response.get("response", str(response))
                eval_count = response.get("eval_count", len(response_text.split()))
                eval_duration = response.get("eval_duration", 0)
            else:
                response_text = str(response)
                eval_count = len(response_text.split())
                eval_duration = 0
            
            return ContextualStepResult(
                step_id=step_id,
                model=step.model,
                output_key=step.output_key,
                response=response_text,
                latency_ms=latency,
                eval_count=eval_count,
                eval_duration_ns=eval_duration,
                success=True,
                metadata=response if isinstance(response, dict) else None
            )
            
        except Exception as e:
            return ContextualStepResult(
                step_id=step_id,
                model=step.model,
                output_key=step.output_key,
                response="",
                latency_ms=0,
                success=False,
                error=str(e)
            )
    
    def _determine_final_answer(self) -> str:
        """ìµœì¢… ë‹µë³€ ê²°ì •"""
        # ê°€ì¥ ë§ˆì§€ë§‰ ì„±ê³µí•œ ë‹¨ê³„ì˜ ê²°ê³¼ë¥¼ ì‚¬ìš©
        for result in reversed(self.results):
            if result.success and not result.skipped and result.response.strip():
                return result.response
        
        # ì»¨í…ìŠ¤íŠ¸ì—ì„œ ê°€ëŠ¥í•œ ë‹µë³€ ì°¾ê¸°
        for key in ["final", "judge", "review", "draft", "answer"]:
            if key in self.context and self.context[key]:
                return self.context[key]
        
        return "No successful result found"
    
    def _step_result_to_dict(self, result: ContextualStepResult) -> Dict[str, Any]:
        """StepResultë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            "step_id": result.step_id,
            "model": result.model,
            "output_key": result.output_key,
            "response": result.response[:100] + "..." if len(result.response) > 100 else result.response,
            "latency_ms": result.latency_ms,
            "eval_count": result.eval_count,
            "success": result.success,
            "skipped": result.skipped,
            "error": result.error
        }

# =============================================================================
# í—¬í¼ í•¨ìˆ˜ë“¤
# =============================================================================

def create_3stage_economic_pipeline() -> ContextualPipeline:
    """3ë‹¨ê³„ ê²½ì œì  ì§€ëŠ¥ íŒŒì´í”„ë¼ì¸ ìƒì„±"""
    
    def should_skip_review(context: Dict) -> bool:
        """ë¦¬ë·° ë‹¨ê³„ë¥¼ ê±´ë„ˆë›¸ì§€ ê²°ì •"""
        draft = context.get("draft", "")
        query = context.get("query", "")
        
        # ê°„ë‹¨í•œ ì§ˆë¬¸ì´ê±°ë‚˜ ì§§ì€ ë‹µë³€ì´ë©´ ê±´ë„ˆë›°ê¸°
        simple_indicators = ["what is", "who is", "when is", "where is", "2+2", "ìˆ˜ë„"]
        is_simple = any(indicator in query.lower() for indicator in simple_indicators)
        is_short = len(draft.split()) < 10
        
        return is_simple and is_short
    
    def should_skip_judge(context: Dict) -> bool:
        """íŒì • ë‹¨ê³„ë¥¼ ê±´ë„ˆë›¸ì§€ ê²°ì •"""
        review = context.get("review", "")
        draft = context.get("draft", "")
        
        # ë¦¬ë·°ê°€ ì—†ê±°ë‚˜ ì´ˆì•ˆê³¼ ë§¤ìš° ìœ ì‚¬í•˜ë©´ ê±´ë„ˆë›°ê¸°
        if not review:
            return True
            
        # ê°„ë‹¨í•œ ìœ ì‚¬ë„ ì²´í¬
        review_words = set(review.lower().split())
        draft_words = set(draft.lower().split())
        
        if len(review_words) > 0:
            similarity = len(review_words & draft_words) / len(review_words | draft_words)
            return similarity > 0.8  # 80% ì´ìƒ ìœ ì‚¬í•˜ë©´ ê±´ë„ˆë›°ê¸°
            
        return False
    
    steps = [
        ContextualPipelineStep(
            model="qwen2:0.5b",
            prompt_template="Answer this question concisely: {query}",
            output_key="draft",
            temperature=0.7
        ),
        ContextualPipelineStep(
            model="gemma:2b", 
            prompt_template="Improve this draft answer:\n\nQuestion: {query}\nDraft: {draft}\n\nImproved answer:",
            output_key="review",
            temperature=0.5,
            promote_if=lambda ctx: not should_skip_review(ctx),
            required_keys=["draft"]
        ),
        ContextualPipelineStep(
            model="llama3:8b",
            prompt_template="Provide the final, highest quality answer:\n\nQuestion: {query}\nDraft: {draft}\nReview: {review}\n\nFinal answer:",
            output_key="final", 
            temperature=0.3,
            promote_if=lambda ctx: not should_skip_judge(ctx),
            required_keys=["draft", "review"]
        )
    ]
    
    return ContextualPipeline("3Stage-Economic-Intelligence", steps)

def run_3stage_with_context(llm_factory, query: str) -> Dict[str, Any]:
    """3ë‹¨ê³„ ì»¨í…ìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í—¬í¼"""
    pipeline = create_3stage_economic_pipeline()
    return pipeline.run(query, llm_factory)

def create_2stage_pipeline() -> ContextualPipeline:
    """2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìƒì„±"""
    steps = [
        ContextualPipelineStep(
            model="qwen2:0.5b",
            prompt_template="Answer briefly: {query}",
            output_key="draft",
            temperature=0.6
        ),
        ContextualPipelineStep(
            model="gemma:2b",
            prompt_template="Improve and finalize:\n\nQuestion: {query}\nDraft: {draft}\n\nFinal answer:",
            output_key="final",
            temperature=0.4,
            required_keys=["draft"]
        )
    ]
    
    return ContextualPipeline("2Stage-Draft-Review", steps)

# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
def test_contextual_pipeline():
    """ì»¨í…ìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    from src.llm.simple_llm import create_llm_auto
    
    print("ğŸ§ª Testing Contextual Pipeline...")
    
    # 3ë‹¨ê³„ í…ŒìŠ¤íŠ¸
    result = run_3stage_with_context(create_llm_auto, "What is the capital of South Korea?")
    
    print(f"Final answer: {result['final']}")
    print(f"Context keys: {list(result['context'].keys())}")
    print(f"Metrics: {result['metrics']}")
    
    return result

if __name__ == "__main__":
    test_contextual_pipeline()