# -*- coding: utf-8 -*-
"""
Project ArkhÄ“ - Perfect Economic Intelligence Pipeline V2
ëª¨ë“  High Priority ì´ìŠˆ í•´ê²°:
- LLM í´ë¼ì´ì–¸íŠ¸ ìž¬ì‚¬ìš©
- Ollama ë©”íƒ€ë°ì´í„° ì§ì ‘ ìˆ˜ì§‘  
- ì—ëŸ¬ ê²©ë¦¬/ë³µêµ¬
- ì™„ì „í•œ ë¡œê¹…/ìž¬í˜„ì„±
- í”„ë¡¬í”„íŠ¸ ìœ„ìƒ (JSON ìŠ¤í‚¤ë§ˆ)
"""

import time
import json
import requests
import numpy as np
import hashlib
from typing import List, Dict, Any, Optional, Tuple, Union
from dataclasses import dataclass, asdict
from pathlib import Path
from datetime import datetime
import logging
import random

@dataclass
class PerfectStageMetrics:
    """ì™„ë²½í•œ ë‹¨ê³„ë³„ ì‹¤ì¸¡ ë©”íŠ¸ë¦­"""
    stage_name: str
    model: str
    start_time: float
    end_time: float
    prompt_hash: str
    seed: int
    temperature: float
    
    # Ollama ì§ì ‘ ë©”íƒ€ë°ì´í„°
    eval_count: int = 0           # ì‹¤ì œ ìƒì„±ëœ í† í° ìˆ˜
    eval_duration: int = 0        # ë‚˜ë…¸ì´ˆ ë‹¨ìœ„ í‰ê°€ ì‹œê°„
    prompt_eval_count: int = 0    # í”„ë¡¬í”„íŠ¸ í† í° ìˆ˜
    prompt_eval_duration: int = 0 # í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ì‹œê°„
    
    # ê³„ì‚°ëœ ë©”íŠ¸ë¦­
    tokens_per_second: float = 0.0
    first_token_latency: float = 0.0  # ms
    total_latency: float = 0.0        # ms
    cost_factor: float = 1.0
    real_cost: float = 0.0
    success: bool = True
    error: Optional[str] = None
    fallback_used: bool = False

@dataclass 
class StructuredSample:
    """êµ¬ì¡°í™”ëœ ìƒ˜í”Œ ê²°ê³¼"""
    raw_text: str
    parsed_json: Optional[Dict] = None
    answer: str = ""
    rationale: str = ""
    confidence: float = 0.5
    tokens: int = 0
    parsing_success: bool = False

@dataclass
class PerfectEconomicResult:
    """ì™„ë²½í•œ ê²½ì œì  ì§€ëŠ¥ ê²°ê³¼"""
    query: str
    query_hash: str
    execution_id: str
    timestamp: str
    
    final_answer: str
    final_confidence: float
    final_rationale: str
    
    total_stages: int
    executed_stages: int
    stage_metrics: List[PerfectStageMetrics]
    entropy_progression: List[float]
    promotion_decisions: List[Tuple[bool, Dict]]
    fallback_count: int
    
    total_cost: float
    total_time: float
    economic_efficiency: float
    cost_saved_ratio: float
    
    # ë¡œê¹… ì •ë³´
    log_file: str
    reproducible: bool = True

class OllamaDirectClient:
    """Ollama REST API ì§ì ‘ í´ë¼ì´ì–¸íŠ¸ - ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘"""
    
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        
    def generate(self, model: str, prompt: str, temperature: float = 0.7, 
                max_tokens: int = 300, seed: int = None) -> Dict[str, Any]:
        """Ollama API ì§ì ‘ í˜¸ì¶œ - ì™„ì „í•œ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘"""
        
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
            }
        }
        
        if seed is not None:
            payload["options"]["seed"] = seed
            
        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=120
            )
            response.raise_for_status()
            
            data = response.json()
            return {
                "response": data.get("response", ""),
                "eval_count": data.get("eval_count", 0),
                "eval_duration": data.get("eval_duration", 0),
                "prompt_eval_count": data.get("prompt_eval_count", 0),  
                "prompt_eval_duration": data.get("prompt_eval_duration", 0),
                "total_duration": data.get("total_duration", 0),
                "model": data.get("model", model),
                "done": data.get("done", True)
            }
            
        except Exception as e:
            return {
                "response": f"ERROR: {str(e)}",
                "error": str(e),
                "eval_count": 0,
                "eval_duration": 0,
                "prompt_eval_count": 0,
                "prompt_eval_duration": 0,
                "total_duration": 0,
                "model": model,
                "done": False
            }

class PromptHygiene:
    """í”„ë¡¬í”„íŠ¸ ìœ„ìƒ - JSON ìŠ¤í‚¤ë§ˆ ê°•ì œ"""
    
    @staticmethod
    def draft_prompt(query: str, seed: int) -> str:
        """ì´ˆì•ˆ í”„ë¡¬í”„íŠ¸ - ê°„ê²°í•œ ë‹µë³€"""
        return f"""Answer this question concisely and clearly:

Question: {query}

Instructions:
- Keep answer under 50 words
- Be factual and direct
- No unnecessary elaboration

Answer:"""

    @staticmethod  
    def review_prompt(query: str, drafts: List[str], seed: int) -> str:
        """ê²€í†  í”„ë¡¬í”„íŠ¸ - JSON ì‘ë‹µ ê°•ì œ"""
        drafts_text = "\n".join([f"Draft {i+1}: {d}" for i, d in enumerate(drafts)])
        
        return f"""Review these draft answers and provide an improved response in exact JSON format:

Question: {query}

{drafts_text}

Respond in this exact JSON format:
{{
  "answer": "your improved answer here",
  "rationale": "why this is better than the drafts", 
  "confidence": 0.75,
  "improvements": ["what you improved"]
}}

JSON Response:"""

    @staticmethod
    def judge_prompt(query: str, drafts: List[str], reviews: List[str], seed: int) -> str:
        """íŒì • í”„ë¡¬í”„íŠ¸ - ì—„ê²©í•œ JSON ìŠ¤í‚¤ë§ˆ"""
        drafts_text = "\n".join([f"Draft {i+1}: {d}" for i, d in enumerate(drafts)])
        reviews_text = "\n".join([f"Review {i+1}: {r}" for i, r in enumerate(reviews)])
        
        return f"""Provide the highest quality final answer by analyzing all previous attempts:

Question: {query}

Previous attempts:
{drafts_text}

{reviews_text}

Respond in this EXACT JSON format:
{{
  "answer": "final authoritative answer",
  "rationale": "detailed reasoning for this answer",
  "confidence": 0.95,
  "sources_analyzed": ["draft", "review"],
  "key_improvements": ["specific improvements made"],
  "quality_score": 9
}}

JSON Response:"""

    @staticmethod
    def parse_json_response(text: str) -> StructuredSample:
        """JSON ì‘ë‹µ íŒŒì‹± - ê°•ë ¥í•œ ì˜¤ë¥˜ ì²˜ë¦¬"""
        sample = StructuredSample(raw_text=text)
        
        try:
            # JSON ì¶”ì¶œ ì‹œë„
            json_start = text.find('{')
            json_end = text.rfind('}') + 1
            
            if json_start >= 0 and json_end > json_start:
                json_str = text[json_start:json_end]
                parsed = json.loads(json_str)
                
                sample.parsed_json = parsed
                sample.answer = parsed.get("answer", "")
                sample.rationale = parsed.get("rationale", "")
                sample.confidence = float(parsed.get("confidence", 0.5))
                sample.parsing_success = True
                
        except Exception as e:
            # JSON íŒŒì‹± ì‹¤íŒ¨ - í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            sample.answer = text.strip()
            sample.rationale = "JSON parsing failed"
            sample.confidence = 0.3  # ë‚®ì€ ì‹ ë¢°ë„
            sample.parsing_success = False
            
        sample.tokens = len(sample.answer.split())
        return sample

class PerfectLogger:
    """ì™„ë²½í•œ ë¡œê¹… ì‹œìŠ¤í…œ"""
    
    def __init__(self, log_dir: str = "logs"):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.log_file = self.log_dir / f"economic_intelligence_{timestamp}.jsonl"
        
        # ì‹¤í—˜ ì„¤ì • ë¡œê¹…
        self.setup_logging()
        
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger('EconomicIntelligence')
        
    def log_stage(self, stage_metrics: PerfectStageMetrics, samples: List[StructuredSample]):
        """ë‹¨ê³„ë³„ ë¡œê¹…"""
        log_entry = {
            "type": "stage",
            "timestamp": datetime.now().isoformat(),
            "stage": asdict(stage_metrics),
            "samples": [
                {
                    "answer": s.answer[:100],  # ì²˜ìŒ 100ìžë§Œ
                    "confidence": s.confidence,
                    "tokens": s.tokens,
                    "parsing_success": s.parsing_success
                } for s in samples
            ]
        }
        
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
    
    def log_result(self, result: PerfectEconomicResult):
        """ìµœì¢… ê²°ê³¼ ë¡œê¹…"""
        log_entry = {
            "type": "final_result",
            "timestamp": datetime.now().isoformat(),
            "result": asdict(result)
        }
        
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')

class PerfectEconomicIntelligence:
    """ì™„ë²½í•œ ê²½ì œì  ì§€ëŠ¥ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, cost_sensitivity: float = 0.3):
        self.cost_sensitivity = cost_sensitivity
        
        # LLM í´ë¼ì´ì–¸íŠ¸ ìž¬ì‚¬ìš© (í•œ ë²ˆë§Œ ìƒì„±)
        self.ollama_client = OllamaDirectClient()
        
        # ëª¨ë¸ë³„ ë¹„ìš© ê³„ìˆ˜
        self.cost_factors = {
            "qwen2:0.5b": 0.8,
            "gemma:2b": 1.0,
            "llama3:8b": 4.0
        }
        
        # í”„ë¡¬í”„íŠ¸ ìœ„ìƒ
        self.prompt_hygiene = PromptHygiene()
        
        # ë¡œê¹…
        self.logger = PerfectLogger()
        
        # ìž¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ
        self.master_seed = random.randint(1000, 9999)
        
    def _generate_with_fallback(self, model: str, prompt: str, temperature: float,
                               max_tokens: int, seed: int, stage_name: str) -> Tuple[StructuredSample, PerfectStageMetrics]:
        """ì—ëŸ¬ ë³µêµ¬ê°€ ìžˆëŠ” ìƒì„±"""
        
        start_time = time.time()
        prompt_hash = hashlib.md5(prompt.encode()).hexdigest()[:8]
        
        metrics = PerfectStageMetrics(
            stage_name=stage_name,
            model=model,
            start_time=start_time,
            end_time=0,
            prompt_hash=prompt_hash,
            seed=seed,
            temperature=temperature,
            cost_factor=self.cost_factors.get(model, 1.0)
        )
        
        try:
            # ë©”ì¸ ì‹œë„
            response = self.ollama_client.generate(
                model=model, 
                prompt=prompt,
                temperature=temperature,
                max_tokens=max_tokens,
                seed=seed
            )
            
            if "error" not in response and response.get("done", False):
                # ì„±ê³µ - Ollama ë©”íƒ€ë°ì´í„° ì§ì ‘ ìˆ˜ì§‘
                metrics.eval_count = response.get("eval_count", 0)
                metrics.eval_duration = response.get("eval_duration", 0)
                metrics.prompt_eval_count = response.get("prompt_eval_count", 0)
                metrics.prompt_eval_duration = response.get("prompt_eval_duration", 0)
                
                # ì‹¤ì œ ê³„ì‚°
                if metrics.eval_duration > 0:
                    metrics.tokens_per_second = metrics.eval_count / (metrics.eval_duration / 1e9)
                
                metrics.end_time = time.time()
                metrics.total_latency = (metrics.end_time - start_time) * 1000
                metrics.real_cost = metrics.eval_count * metrics.cost_factor / 1000
                metrics.success = True
                
                # êµ¬ì¡°í™”ëœ íŒŒì‹±
                sample = self.prompt_hygiene.parse_json_response(response["response"])
                
                return sample, metrics
                
        except Exception as e:
            metrics.error = str(e)
            
        # í´ë°± - ê°„ë‹¨í•œ ì‘ë‹µ
        metrics.end_time = time.time()
        metrics.total_latency = (metrics.end_time - start_time) * 1000
        metrics.success = False
        metrics.fallback_used = True
        metrics.real_cost = 0.1  # í´ë°± ë¹„ìš©
        
        fallback_sample = StructuredSample(
            raw_text="FALLBACK: Unable to generate response",
            answer="Error in generation",
            confidence=0.1,
            tokens=5
        )
        
        return fallback_sample, metrics
    
    def _multi_sample_stage(self, model: str, prompt: str, stage_name: str, 
                           n_samples: int, temperature: float) -> Tuple[List[StructuredSample], PerfectStageMetrics]:
        """ë‹¤ì¤‘ ìƒ˜í”Œë§ with ì—ëŸ¬ ë³µêµ¬"""
        
        samples = []
        all_metrics = []
        
        for i in range(n_samples):
            seed = self.master_seed + i  # ìž¬í˜„ ê°€ëŠ¥í•œ ì‹œë“œ
            sample, metrics = self._generate_with_fallback(
                model, prompt, temperature, 300, seed, f"{stage_name}_sample_{i+1}"
            )
            samples.append(sample)
            all_metrics.append(metrics)
        
        # ì§‘ê³„ëœ ë©”íŠ¸ë¦­
        total_metrics = PerfectStageMetrics(
            stage_name=stage_name,
            model=model,
            start_time=min(m.start_time for m in all_metrics),
            end_time=max(m.end_time for m in all_metrics),
            prompt_hash=all_metrics[0].prompt_hash,
            seed=self.master_seed,
            temperature=temperature,
            eval_count=sum(m.eval_count for m in all_metrics),
            eval_duration=sum(m.eval_duration for m in all_metrics),
            prompt_eval_count=sum(m.prompt_eval_count for m in all_metrics),
            prompt_eval_duration=sum(m.prompt_eval_duration for m in all_metrics),
            cost_factor=self.cost_factors.get(model, 1.0),
            real_cost=sum(m.real_cost for m in all_metrics),
            total_latency=sum(m.total_latency for m in all_metrics),
            success=any(m.success for m in all_metrics),
            fallback_used=any(m.fallback_used for m in all_metrics)
        )
        
        if total_metrics.eval_duration > 0:
            total_metrics.tokens_per_second = total_metrics.eval_count / (total_metrics.eval_duration / 1e9)
        
        # ë¡œê¹…
        self.logger.log_stage(total_metrics, samples)
        
        return samples, total_metrics
    
    def execute(self, query: str) -> PerfectEconomicResult:
        """ì™„ë²½í•œ ê²½ì œì  ì§€ëŠ¥ ì‹¤í–‰"""
        
        execution_id = f"ei_{int(time.time())}_{random.randint(1000,9999)}"
        query_hash = hashlib.md5(query.encode()).hexdigest()[:8]
        
        print(f"\nðŸ§  Perfect Economic Intelligence: {query}")
        print(f"Execution ID: {execution_id}")
        print(f"Master seed: {self.master_seed}")
        
        stage_metrics = []
        entropy_progression = []
        promotion_decisions = []
        fallback_count = 0
        
        # Stage 1: Draft (qwen2:0.5b)
        print("\nðŸ“ Stage 1 (Draft): qwen2:0.5b - Multi-sampling...")
        draft_prompt = self.prompt_hygiene.draft_prompt(query, self.master_seed)
        draft_samples, draft_metrics = self._multi_sample_stage(
            "qwen2:0.5b", draft_prompt, "draft", n_samples=5, temperature=0.8
        )
        stage_metrics.append(draft_metrics)
        
        if draft_metrics.fallback_used:
            fallback_count += 1
            
        # ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
        draft_texts = [s.answer for s in draft_samples if s.answer]
        draft_entropy = self._calculate_entropy(draft_texts) if draft_texts else 0
        entropy_progression.append(draft_entropy)
        
        print(f"  Samples: {len(draft_samples)}, Entropy: {draft_entropy:.3f}")
        print(f"  Tokens/s: {draft_metrics.tokens_per_second:.1f}, Cost: ${draft_metrics.real_cost:.4f}")
        
        # ìŠ¹ê¸‰ ê²°ì • 1
        promote_1, promo_1_info = self._promotion_decision(draft_samples, 0.8, 1.0, draft_entropy)
        promotion_decisions.append((promote_1, promo_1_info))
        
        if not promote_1:
            print(f"  ðŸ›‘ Early termination: {promo_1_info}")
            best_draft = max(draft_samples, key=lambda x: x.confidence)
            return self._create_result(query, execution_id, query_hash, best_draft.answer,
                                     best_draft.confidence, best_draft.rationale, 1, 1,
                                     stage_metrics, entropy_progression, promotion_decisions, fallback_count)
        
        # Stage 2: Review (gemma:2b) 
        print("\nðŸ” Stage 2 (Review): gemma:2b - JSON structured...")
        top_drafts = sorted(draft_samples, key=lambda x: x.confidence, reverse=True)[:3]
        review_prompt = self.prompt_hygiene.review_prompt(
            query, [d.answer for d in top_drafts], self.master_seed + 100
        )
        review_samples, review_metrics = self._multi_sample_stage(
            "gemma:2b", review_prompt, "review", n_samples=3, temperature=0.5
        )
        stage_metrics.append(review_metrics)
        
        if review_metrics.fallback_used:
            fallback_count += 1
        
        review_texts = [s.answer for s in review_samples if s.answer]
        review_entropy = self._calculate_entropy(review_texts) if review_texts else draft_entropy
        entropy_progression.append(review_entropy)
        
        print(f"  Samples: {len(review_samples)}, Entropy: {review_entropy:.3f}")
        print(f"  JSON parsing: {sum(1 for s in review_samples if s.parsing_success)}/{len(review_samples)}")
        
        # ìŠ¹ê¸‰ ê²°ì • 2
        promote_2, promo_2_info = self._promotion_decision(review_samples, 1.0, 4.0, review_entropy)
        promotion_decisions.append((promote_2, promo_2_info))
        
        if not promote_2:
            print(f"  ðŸ›‘ Early termination: {promo_2_info}")
            best_review = max(review_samples, key=lambda x: x.confidence)
            return self._create_result(query, execution_id, query_hash, best_review.answer,
                                     best_review.confidence, best_review.rationale, 2, 2,
                                     stage_metrics, entropy_progression, promotion_decisions, fallback_count)
        
        # Stage 3: Judge (llama3:8b)
        print("\nâš–ï¸  Stage 3 (Judge): llama3:8b - Final judgment...")
        top_reviews = sorted(review_samples, key=lambda x: x.confidence, reverse=True)[:2]
        judge_prompt = self.prompt_hygiene.judge_prompt(
            query, [d.answer for d in top_drafts[:2]], [r.answer for r in top_reviews], self.master_seed + 200
        )
        judge_samples, judge_metrics = self._multi_sample_stage(
            "llama3:8b", judge_prompt, "judge", n_samples=2, temperature=0.2
        )
        stage_metrics.append(judge_metrics)
        
        if judge_metrics.fallback_used:
            fallback_count += 1
            
        judge_texts = [s.answer for s in judge_samples if s.answer]
        judge_entropy = self._calculate_entropy(judge_texts) if judge_texts else review_entropy
        entropy_progression.append(judge_entropy)
        
        print(f"  Samples: {len(judge_samples)}, Entropy: {judge_entropy:.3f}")
        print(f"  JSON parsing: {sum(1 for s in judge_samples if s.parsing_success)}/{len(judge_samples)}")
        
        # ìµœì¢… ê²°ê³¼
        if judge_samples:
            best_judge = max(judge_samples, key=lambda x: x.confidence)
            final_answer = best_judge.answer
            final_confidence = best_judge.confidence
            final_rationale = best_judge.rationale
        else:
            # Judge ì™„ì „ ì‹¤íŒ¨ - Review í´ë°±
            best_review = max(review_samples, key=lambda x: x.confidence)
            final_answer = best_review.answer
            final_confidence = best_review.confidence * 0.8  # íŽ˜ë„í‹°
            final_rationale = "Judge failed, using review result"
            fallback_count += 1
            
        return self._create_result(query, execution_id, query_hash, final_answer,
                                 final_confidence, final_rationale, 3, 3,
                                 stage_metrics, entropy_progression, promotion_decisions, fallback_count)
    
    def _calculate_entropy(self, texts: List[str]) -> float:
        """Shannon Entropy ê³„ì‚°"""
        if not texts:
            return 0.0
            
        word_counts = []
        for text in texts:
            words = text.lower().split()
            word_counts.extend(words)
        
        if not word_counts:
            return 0.0
            
        from collections import Counter
        counter = Counter(word_counts)
        total = sum(counter.values())
        
        entropy = 0.0
        for count in counter.values():
            p = count / total
            if p > 0:
                entropy -= p * np.log2(p)
                
        return entropy
    
    def _promotion_decision(self, samples: List[StructuredSample], current_cost: float, 
                           next_cost: float, entropy: float) -> Tuple[bool, Dict]:
        """ìŠ¹ê¸‰ ê²°ì •"""
        if not samples:
            return True, {"reason": "no_samples"}
            
        avg_confidence = np.mean([s.confidence for s in samples])
        cost_ratio = next_cost / current_cost
        
        # ìŠ¹ê¸‰ ì¡°ê±´
        high_entropy = entropy > 2.0
        low_confidence = avg_confidence < 0.7
        cost_effective = (entropy * (1 - avg_confidence)) > (self.cost_sensitivity * cost_ratio)
        
        should_promote = high_entropy or low_confidence or cost_effective
        
        return should_promote, {
            "entropy": entropy,
            "confidence": avg_confidence,
            "cost_ratio": cost_ratio,
            "high_entropy": high_entropy,
            "low_confidence": low_confidence,
            "cost_effective": cost_effective
        }
    
    def _create_result(self, query: str, execution_id: str, query_hash: str,
                      final_answer: str, final_confidence: float, final_rationale: str,
                      total_stages: int, executed_stages: int,
                      stage_metrics: List[PerfectStageMetrics], 
                      entropy_progression: List[float],
                      promotion_decisions: List[Tuple[bool, Dict]],
                      fallback_count: int) -> PerfectEconomicResult:
        """ì™„ë²½í•œ ê²°ê³¼ ìƒì„±"""
        
        total_cost = sum(m.real_cost for m in stage_metrics)
        total_time = sum(m.total_latency for m in stage_metrics)
        
        # ê²½ì œì  íš¨ìœ¨ì„±
        utility = final_confidence * len(final_answer) / 100
        economic_efficiency = (utility - self.cost_sensitivity * total_cost) / max(total_time/1000, 0.001)
        
        # ë¹„ìš© ì ˆì•½
        max_cost = sum(self.cost_factors.values()) * 3  # ìµœëŒ€ ê°€ëŠ¥ ë¹„ìš©
        cost_saved_ratio = 1 - (total_cost / max_cost) if max_cost > 0 else 0
        
        result = PerfectEconomicResult(
            query=query,
            query_hash=query_hash,
            execution_id=execution_id,
            timestamp=datetime.now().isoformat(),
            final_answer=final_answer,
            final_confidence=final_confidence,
            final_rationale=final_rationale,
            total_stages=total_stages,
            executed_stages=executed_stages,
            stage_metrics=stage_metrics,
            entropy_progression=entropy_progression,
            promotion_decisions=promotion_decisions,
            fallback_count=fallback_count,
            total_cost=total_cost,
            total_time=total_time,
            economic_efficiency=economic_efficiency,
            cost_saved_ratio=cost_saved_ratio,
            log_file=str(self.logger.log_file),
            reproducible=True
        )
        
        # ìµœì¢… ë¡œê¹…
        self.logger.log_result(result)
        
        return result