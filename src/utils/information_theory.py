#!/usr/bin/env python3
"""
Project ArkhÄ“ - Information Theory Module
Shannon Entropy ê¸°ë°˜ ì •ë³´ ì†ì‹¤ ì¸¡ì • ë° ìµœì í™”
"""

import math
import re
from typing import List, Dict, Any, Tuple
from collections import Counter
import numpy as np
from dataclasses import dataclass

@dataclass
class EntropyMetrics:
    """ì—”íŠ¸ë¡œí”¼ ì¸¡ì • ê²°ê³¼"""
    shannon_entropy: float
    unique_words: int
    total_words: int
    compression_ratio: float
    information_density: float

@dataclass
class InformationFlow:
    """ë‹¨ê³„ë³„ ì •ë³´ íë¦„ ë¶„ì„"""
    stage_name: str
    input_entropy: float
    output_entropy: float
    information_gain: float  # positive = gain, negative = loss
    compression_efficiency: float
    
class ShannonEntropyAnalyzer:
    """Shannon Entropy ê¸°ë°˜ ì •ë³´ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.stage_history = []
        
    def calculate_shannon_entropy(self, text: str) -> float:
        """í…ìŠ¤íŠ¸ì˜ Shannon Entropy ê³„ì‚°"""
        if not text or len(text) == 0:
            return 0.0
            
        # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ì„ (ë” ì˜ë¯¸ìˆëŠ” ì •ë³´ ë‹¨ìœ„)
        words = self._preprocess_text(text)
        if len(words) == 0:
            return 0.0
            
        # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
        word_counts = Counter(words)
        total_words = len(words)
        
        # Shannon Entropy ê³„ì‚°: H(X) = -Î£ p(x) * log2(p(x))
        entropy = 0.0
        for word, count in word_counts.items():
            probability = count / total_words
            if probability > 0:
                entropy -= probability * math.log2(probability)
                
        return entropy
    
    def _preprocess_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ ì¶”ì¶œ"""
        # ì†Œë¬¸ì ë³€í™˜ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text.lower())
        words = text.split()
        
        # ë¶ˆìš©ì–´ ì œê±° (ê°„ë‹¨í•œ ë²„ì „)
        stopwords = {'ì˜', 'ê°€', 'ì´', 'ì€', 'ëŠ”', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ê³¼', 'ì™€', 'ê·¸ë¦¬ê³ ', 'ë˜í•œ', 'í•˜ì§€ë§Œ',
                    'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        
        meaningful_words = [word for word in words if len(word) > 1 and word not in stopwords]
        return meaningful_words
    
    def analyze_text_diversity(self, text: str) -> EntropyMetrics:
        """í…ìŠ¤íŠ¸ ë‹¤ì–‘ì„± ì¢…í•© ë¶„ì„"""
        words = self._preprocess_text(text)
        unique_words = len(set(words))
        total_words = len(words)
        
        shannon_entropy = self.calculate_shannon_entropy(text)
        
        # ì••ì¶• ë¹„ìœ¨ (ìœ ë‹ˆí¬ ë‹¨ì–´ / ì „ì²´ ë‹¨ì–´)
        compression_ratio = unique_words / total_words if total_words > 0 else 0
        
        # ì •ë³´ ë°€ë„ (ì—”íŠ¸ë¡œí”¼ / í…ìŠ¤íŠ¸ ê¸¸ì´)
        information_density = shannon_entropy / len(text) if len(text) > 0 else 0
        
        return EntropyMetrics(
            shannon_entropy=shannon_entropy,
            unique_words=unique_words,
            total_words=total_words,
            compression_ratio=compression_ratio,
            information_density=information_density
        )
    
    def track_information_flow(self, stage_name: str, input_text: str, output_text: str) -> InformationFlow:
        """ë‹¨ê³„ë³„ ì •ë³´ íë¦„ ì¶”ì """
        input_entropy = self.calculate_shannon_entropy(input_text)
        output_entropy = self.calculate_shannon_entropy(output_text)
        
        # ì •ë³´ íšë“/ì†ì‹¤ ê³„ì‚°
        information_gain = output_entropy - input_entropy
        
        # ì••ì¶• íš¨ìœ¨ì„± (ì¶œë ¥ ì—”íŠ¸ë¡œí”¼ / ì…ë ¥ ì—”íŠ¸ë¡œí”¼)
        compression_efficiency = output_entropy / input_entropy if input_entropy > 0 else 1.0
        
        flow = InformationFlow(
            stage_name=stage_name,
            input_entropy=input_entropy,
            output_entropy=output_entropy,
            information_gain=information_gain,
            compression_efficiency=compression_efficiency
        )
        
        self.stage_history.append(flow)
        return flow
    
    def analyze_pipeline_efficiency(self) -> Dict[str, Any]:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì •ë³´ íš¨ìœ¨ì„± ë¶„ì„"""
        if not self.stage_history:
            return {"error": "No stage history available"}
            
        # ì „ì²´ ì •ë³´ íë¦„ ë¶„ì„
        initial_entropy = self.stage_history[0].input_entropy
        final_entropy = self.stage_history[-1].output_entropy
        
        total_information_gain = final_entropy - initial_entropy
        total_stages = len(self.stage_history)
        
        # ë‹¨ê³„ë³„ ì •ë³´ ì†ì‹¤/íšë“
        information_changes = []
        for flow in self.stage_history:
            information_changes.append({
                "stage": flow.stage_name,
                "input_entropy": flow.input_entropy,
                "output_entropy": flow.output_entropy,
                "information_gain": flow.information_gain,
                "efficiency": flow.compression_efficiency
            })
        
        # íŒŒì´í”„ë¼ì¸ íš¨ìœ¨ì„± ë©”íŠ¸ë¦­
        average_efficiency = sum(flow.compression_efficiency for flow in self.stage_history) / total_stages
        information_preservation_rate = final_entropy / initial_entropy if initial_entropy > 0 else 1.0
        
        return {
            "initial_entropy": initial_entropy,
            "final_entropy": final_entropy,
            "total_information_gain": total_information_gain,
            "information_preservation_rate": information_preservation_rate,
            "average_stage_efficiency": average_efficiency,
            "stage_analysis": information_changes,
            "pipeline_verdict": self._get_pipeline_verdict(information_preservation_rate, average_efficiency)
        }
    
    def _get_pipeline_verdict(self, preservation_rate: float, avg_efficiency: float) -> str:
        """íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ íŒì •"""
        if preservation_rate > 1.2 and avg_efficiency > 1.1:
            return "EXCELLENT: Information gain with high efficiency"
        elif preservation_rate > 1.0 and avg_efficiency > 1.0:
            return "GOOD: Positive information flow"
        elif preservation_rate > 0.8:
            return "ACCEPTABLE: Minor information loss"
        else:
            return "POOR: Significant information loss detected"
    
    def suggest_optimization(self) -> List[str]:
        """ì •ë³´ ì´ë¡  ê¸°ë°˜ ìµœì í™” ì œì•ˆ"""
        if not self.stage_history:
            return ["No data available for optimization"]
            
        suggestions = []
        
        # ì •ë³´ ì†ì‹¤ì´ í° ë‹¨ê³„ ì‹ë³„
        for flow in self.stage_history:
            if flow.information_gain < -1.0:  # ì‹¬ê°í•œ ì •ë³´ ì†ì‹¤
                suggestions.append(f"âš ï¸ {flow.stage_name} stage shows significant information loss (-{abs(flow.information_gain):.2f})")
                suggestions.append(f"   â†’ Consider: Richer prompts, larger model, or better context preservation")
            
            if flow.compression_efficiency < 0.7:  # ë¹„íš¨ìœ¨ì  ì••ì¶•
                suggestions.append(f"ğŸ“‰ {flow.stage_name} stage has low efficiency ({flow.compression_efficiency:.2f})")
                suggestions.append(f"   â†’ Consider: Simplify prompts or use more capable model")
        
        # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì œì•ˆ
        analysis = self.analyze_pipeline_efficiency()
        if analysis["information_preservation_rate"] < 0.8:
            suggestions.append("ğŸ”§ Overall pipeline loses too much information")
            suggestions.append("   â†’ Consider: Reduce compression, improve context passing")
        
        if analysis["average_stage_efficiency"] > 1.3:
            suggestions.append("âœ¨ Pipeline shows good information amplification")
            suggestions.append("   â†’ Consider: Apply this approach to other problem types")
        
        return suggestions if suggestions else ["âœ… Pipeline shows good information efficiency"]

class EntropyBasedOptimizer:
    """ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ Multi-Agent ìµœì í™”"""
    
    def __init__(self, analyzer: ShannonEntropyAnalyzer):
        self.analyzer = analyzer
        
    def optimize_stage_prompts(self, current_prompt: str, target_entropy: float) -> str:
        """ëª©í‘œ ì—”íŠ¸ë¡œí”¼ì— ë§ê²Œ í”„ë¡¬í”„íŠ¸ ìµœì í™”"""
        current_entropy = self.analyzer.calculate_shannon_entropy(current_prompt)
        
        if current_entropy < target_entropy:
            # ì—”íŠ¸ë¡œí”¼ ì¦ê°€ í•„ìš” - ë” ë‹¤ì–‘í•œ ê´€ì  ì¶”ê°€
            enhanced_prompt = f"""
{current_prompt}

ë‹¤ìŒ ê´€ì ë“¤ì„ ì¶”ê°€ë¡œ ê³ ë ¤í•˜ì„¸ìš”:
- ëŒ€ì•ˆì  ì ‘ê·¼ ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€?
- ì˜ˆìƒì¹˜ ëª»í•œ ê²°ê³¼ë‚˜ ë¶€ì‘ìš©ì€?
- ë‹¤ë¥¸ ë¶„ì•¼ì˜ ìœ ì‚¬í•œ ì‚¬ë¡€ëŠ”?
- ì°½ì˜ì ì´ê³  í˜ì‹ ì ì¸ ì•„ì´ë””ì–´ëŠ”?
"""
            return enhanced_prompt.strip()
        else:
            # ì—”íŠ¸ë¡œí”¼ ì ì • - í˜„ì¬ í”„ë¡¬í”„íŠ¸ ìœ ì§€
            return current_prompt
    
    def calculate_optimal_model_allocation(self, problem_complexity: float) -> Dict[str, str]:
        """ë¬¸ì œ ë³µì¡ë„ì— ë”°ë¥¸ ìµœì  ëª¨ë¸ ë°°ì¹˜"""
        
        # ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ ëª¨ë¸ ì„ íƒ ê·œì¹™
        if problem_complexity < 3.0:
            return {
                "draft": "qwen2:0.5b",    # ë‚®ì€ ì—”íŠ¸ë¡œí”¼ ë¬¸ì œëŠ” ë¹ ë¥¸ ëª¨ë¸
                "review": None,           # Review ë‹¨ê³„ ìƒëµ
                "judge": "qwen2:7b"       # ì ë‹¹í•œ ì„±ëŠ¥ ëª¨ë¸
            }
        elif problem_complexity < 7.0:
            return {
                "draft": "qwen2:7b",      # ì¤‘ê°„ ë³µì¡ë„ëŠ” ê· í˜•ì¡íŒ ëª¨ë¸
                "review": "qwen2:7b",     
                "judge": "llama3:8b"      # ìµœì¢…ì€ ê°•ë ¥í•œ ëª¨ë¸
            }
        else:
            return {
                "draft": "qwen2:7b",      # ê³ ë³µì¡ë„ëŠ” ëª¨ë“  ë‹¨ê³„ ê°•í™”
                "review": "llama3:8b",    
                "judge": "llama3:8b"      # ë˜ëŠ” ë” í° ëª¨ë¸
            }

# ì‚¬ìš© ì˜ˆì‹œ
def demonstrate_entropy_analysis():
    """ì—”íŠ¸ë¡œí”¼ ë¶„ì„ ë°ëª¨"""
    analyzer = ShannonEntropyAnalyzer()
    
    # ìƒ˜í”Œ í…ìŠ¤íŠ¸ë“¤
    simple_text = "ì„œìš¸ì€ ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ì…ë‹ˆë‹¤."
    complex_text = """
    ë¯¸ë˜ ë„ì‹œì˜ êµí†µ ì²´ì¦ í•´ê²°ì„ ìœ„í•´ì„œëŠ” ë‹¤ì¸µì  ì ‘ê·¼ì´ í•„ìš”í•©ë‹ˆë‹¤. 
    ì²«ì§¸, 3ì°¨ì› êµí†µ ì‹œìŠ¤í…œì„ í†µí•´ ì§€í•˜-ì§€ìƒ-ê³µì¤‘ì„ ì—°ê²°í•œ ì…ì²´ì  êµí†µë§ì„ êµ¬ì¶•í•´ì•¼ í•©ë‹ˆë‹¤.
    ë‘˜ì§¸, AI ê¸°ë°˜ ì˜ˆì¸¡ ì‹ í˜¸ë“± ì‹œìŠ¤í…œìœ¼ë¡œ ì‹¤ì‹œê°„ êµí†µëŸ‰ì„ ë¶„ì„í•˜ì—¬ ì‹ í˜¸ë¥¼ ìµœì í™”í•´ì•¼ í•©ë‹ˆë‹¤.
    ì…‹ì§¸, ê°œì¸ìš© ë“œë¡  íƒì‹œì™€ ê°™ì€ ìƒˆë¡œìš´ êµí†µìˆ˜ë‹¨ì„ ë„ì…í•˜ì—¬ ë‹¨ê±°ë¦¬ ì´ë™ì˜ íš¨ìœ¨ì„±ì„ ë†’ì—¬ì•¼ í•©ë‹ˆë‹¤.
    """
    
    # ì—”íŠ¸ë¡œí”¼ ë¶„ì„
    simple_metrics = analyzer.analyze_text_diversity(simple_text)
    complex_metrics = analyzer.analyze_text_diversity(complex_text)
    
    print("=== Shannon Entropy Analysis Demo ===")
    print(f"\nSimple text entropy: {simple_metrics.shannon_entropy:.2f}")
    print(f"Complex text entropy: {complex_metrics.shannon_entropy:.2f}")
    print(f"Entropy ratio: {complex_metrics.shannon_entropy / simple_metrics.shannon_entropy:.2f}x")
    
    # ì •ë³´ íë¦„ ì¶”ì 
    flow = analyzer.track_information_flow("Enhancement", simple_text, complex_text)
    print(f"\nInformation gain: {flow.information_gain:.2f}")
    print(f"Compression efficiency: {flow.compression_efficiency:.2f}")

if __name__ == "__main__":
    demonstrate_entropy_analysis()