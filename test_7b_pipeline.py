# -*- coding: utf-8 -*-
"""
7B ëª¨ë¸ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
ëª¨ë“  Agentë¥¼ qwen2:7bë¡œ ì—…ê·¸ë ˆì´ë“œí•˜ì—¬ ì„±ëŠ¥ ì¬ì¸¡ì •
"""

import sys
import time
import json
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
sys.path.append('.')

from src.llm.simple_llm import create_llm_auto
from src.orchestrator.thought_aggregator import ThoughtAggregator

@dataclass
class Model7BResult:
    """7B ëª¨ë¸ ì‹¤í–‰ ê²°ê³¼"""
    approach: str
    question: str
    draft_responses: List[str]
    review_responses: List[str] 
    judge_response: str
    total_tokens: int
    total_time_ms: int
    accuracy_score: float
    success: bool
    error: Optional[str] = None

class Pipeline7BTester:
    """7B ëª¨ë¸ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        # ëª¨ë“  Agentë¥¼ 7B ëª¨ë¸ë¡œ ì„¤ì •
        self.draft_llm = create_llm_auto("qwen2:7b")
        self.review_llm = create_llm_auto("qwen2:7b") 
        self.judge_llm = create_llm_auto("qwen2:7b")
        self.aggregator = ThoughtAggregator(model_name="qwen2:7b")
    
    def run_7b_a_approach(self, question: str, expected_answer: str) -> Model7BResult:
        """Aì•ˆ 7B: ThoughtAggregator ì‚¬ìš©"""
        
        start_time = time.time()
        total_tokens = 0
        
        try:
            print(f"  Draft ë‹¨ê³„ ì‹œì‘...")
            # 1. Draft ë‹¨ê³„ (7B ëª¨ë¸)
            draft_responses = []
            for i in range(3):
                prompt = f"ì§ˆë¬¸: {question}\n\në‹µë³€í•˜ê³  ì‚¬ê³ ê³¼ì •ì„ ì„¤ëª…í•˜ì„¸ìš”:"
                response = self.draft_llm.generate(prompt, temperature=0.3 + i*0.1, max_tokens=200)
                
                if isinstance(response, dict):
                    draft_text = response.get("response", "").strip()
                else:
                    draft_text = str(response).strip()
                
                draft_responses.append(draft_text)
                total_tokens += len(prompt.split()) + len(draft_text.split())
                print(f"    Draft {i+1} ì™„ë£Œ")
            
            print(f"  Review ë‹¨ê³„ ì‹œì‘...")
            # 2. Review ë‹¨ê³„ (7B + ThoughtAggregator)
            review_responses = []
            for reviewer_id in range(2):
                # ì‚¬ê³ ê³¼ì • ì••ì¶•
                analysis = self.aggregator.analyze_thoughts(draft_responses, question)
                
                # ì••ì¶•ëœ ì»¨í…ìŠ¤íŠ¸ë¡œ ë¦¬ë·°
                review_prompt = f"""ì••ì¶•ëœ ì‚¬ê³ ê³¼ì •ì„ ë¶„ì„í•˜ì—¬ ë¦¬ë·°í•˜ì„¸ìš”:

{analysis.compressed_context}

ì§ˆë¬¸: {question}
ê²€í† ì ê´€ì  {reviewer_id + 1}ì—ì„œ:
1. ì œì‹œëœ ì •ë³´ì˜ ì •í™•ì„± ê²€ì¦
2. ëˆ„ë½ëœ ì¤‘ìš”í•œ ê´€ì ì´ë‚˜ ì •ë³´ í™•ì¸
3. ë” ë‚˜ì€ ë‹µë³€ì„ ìœ„í•œ ê°œì„  ì‚¬í•­ ì œì‹œ

ë¦¬ë·° ê²°ê³¼:"""

                response = self.review_llm.generate(review_prompt, temperature=0.4, max_tokens=250)
                
                if isinstance(response, dict):
                    review_text = response.get("response", "").strip()
                else:
                    review_text = str(response).strip()
                
                review_responses.append(review_text)
                total_tokens += len(review_prompt.split()) + len(review_text.split())
                print(f"    Review {reviewer_id+1} ì™„ë£Œ")
            
            print(f"  Judge ë‹¨ê³„ ì‹œì‘...")
            # 3. Judge ë‹¨ê³„ (7B)
            judge_prompt = f"""ì§ˆë¬¸: {question}

Draft ì›ë³¸ë“¤:
{chr(10).join(f"Draft {i+1}: {draft}" for i, draft in enumerate(draft_responses))}

Review ë¶„ì„ë“¤:
Review 1: {review_responses[0]}
Review 2: {review_responses[1]}

ì‘ì—… ì§€ì‹œ:
1. ì›ë˜ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µí•˜ëŠ” ê²ƒì´ ìµœìš°ì„ ì…ë‹ˆë‹¤
2. Draftë“¤ê³¼ Reviewë“¤ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”
3. ì •í™•í•œ ì •ë³´ëŠ” ì±„íƒí•˜ê³  ì˜ëª»ëœ ì •ë³´ëŠ” ë°°ì œí•˜ì„¸ìš”
4. ê°„ê²°í•˜ê³  ì •í™•í•œ ìµœì¢… ë‹µë³€ì„ ì œì‹œí•˜ì„¸ìš”

ìµœì¢… ë‹µë³€:"""

            judge_response = self.judge_llm.generate(judge_prompt, temperature=0.2, max_tokens=200)
            
            if isinstance(judge_response, dict):
                judge_text = judge_response.get("response", "").strip()
            else:
                judge_text = str(judge_response).strip()
                
            total_tokens += len(judge_prompt.split()) + len(judge_text.split())
            print(f"    Judge ì™„ë£Œ")
            
            # ì •í™•ë„ í‰ê°€
            accuracy = self._evaluate_accuracy(judge_text, expected_answer)
            
            return Model7BResult(
                approach="Aì•ˆ 7B (ThoughtAggregator)",
                question=question,
                draft_responses=draft_responses,
                review_responses=review_responses,
                judge_response=judge_text,
                total_tokens=total_tokens,
                total_time_ms=int((time.time() - start_time) * 1000),
                accuracy_score=accuracy,
                success=True
            )
            
        except Exception as e:
            return Model7BResult(
                approach="Aì•ˆ 7B (ThoughtAggregator)",
                question=question,
                draft_responses=[],
                review_responses=[],
                judge_response="",
                total_tokens=0,
                total_time_ms=0,
                accuracy_score=0.0,
                success=False,
                error=str(e)
            )
    
    def run_7b_b_approach(self, question: str, expected_answer: str) -> Model7BResult:
        """Bì•ˆ 7B: í”„ë¡¬í”„íŠ¸ ê°œì„ """
        
        start_time = time.time()
        total_tokens = 0
        
        try:
            print(f"  Draft ë‹¨ê³„ ì‹œì‘...")
            # 1. Draft ë‹¨ê³„ (ë™ì¼)
            draft_responses = []
            for i in range(3):
                prompt = f"ì§ˆë¬¸: {question}\n\në‹µë³€í•˜ê³  ì‚¬ê³ ê³¼ì •ì„ ì„¤ëª…í•˜ì„¸ìš”:"
                response = self.draft_llm.generate(prompt, temperature=0.3 + i*0.1, max_tokens=200)
                
                if isinstance(response, dict):
                    draft_text = response.get("response", "").strip()
                else:
                    draft_text = str(response).strip()
                
                draft_responses.append(draft_text)
                total_tokens += len(prompt.split()) + len(draft_text.split())
                print(f"    Draft {i+1} ì™„ë£Œ")
            
            print(f"  Review ë‹¨ê³„ ì‹œì‘...")
            # 2. Review ë‹¨ê³„ (7B + ê°œì„ ëœ í”„ë¡¬í”„íŠ¸)
            review_responses = []
            for reviewer_id in range(2):
                review_prompt = f"""ì§ˆë¬¸: {question}

Draft ë‹µë³€ë“¤ì„ ë¶„ì„í•˜ì—¬ ë¦¬ë·°í•˜ì„¸ìš”:
{chr(10).join(f"Draft {i+1}: {resp}" for i, resp in enumerate(draft_responses))}

ê²€í† ì {reviewer_id + 1} ê´€ì ì—ì„œ:
1. ê³µí†µ í•µì‹¬ ë‚´ìš© íŒŒì•…: ëª¨ë“  Draftê°€ ë™ì˜í•˜ëŠ” ë¶€ë¶„
2. ì°¨ë³„ì  ë¶„ì„: ê° Draftë§Œì˜ ë…íŠ¹í•œ ì ‘ê·¼ì´ë‚˜ ì •ë³´
3. ì •í™•ì„± ê²€ì¦: ì‚¬ì‹¤ì  ì˜¤ë¥˜ë‚˜ ë…¼ë¦¬ì  ë¬¸ì œì  ì‹ë³„
4. í†µí•© ê°œì„ ì•ˆ: ê°€ì¥ ì •í™•í•˜ê³  ì™„ì „í•œ ë‹µë³€ ì œì‹œ

ë¶„ì„ ê²°ê³¼:"""

                response = self.review_llm.generate(review_prompt, temperature=0.4, max_tokens=250)
                
                if isinstance(response, dict):
                    review_text = response.get("response", "").strip()
                else:
                    review_text = str(response).strip()
                
                review_responses.append(review_text)
                total_tokens += len(review_prompt.split()) + len(review_text.split())
                print(f"    Review {reviewer_id+1} ì™„ë£Œ")
            
            print(f"  Judge ë‹¨ê³„ ì‹œì‘...")
            # 3. Judge ë‹¨ê³„ (ë™ì¼í•œ í”„ë¡¬í”„íŠ¸)
            judge_prompt = f"""ì§ˆë¬¸: {question}

Draft ì›ë³¸ë“¤:
{chr(10).join(f"Draft {i+1}: {draft}" for i, draft in enumerate(draft_responses))}

Review ë¶„ì„ë“¤:
Review 1: {review_responses[0]}
Review 2: {review_responses[1]}

ì‘ì—… ì§€ì‹œ:
1. ì›ë˜ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µí•˜ëŠ” ê²ƒì´ ìµœìš°ì„ ì…ë‹ˆë‹¤
2. Draftë“¤ê³¼ Reviewë“¤ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”
3. ì •í™•í•œ ì •ë³´ëŠ” ì±„íƒí•˜ê³  ì˜ëª»ëœ ì •ë³´ëŠ” ë°°ì œí•˜ì„¸ìš”
4. ê°„ê²°í•˜ê³  ì •í™•í•œ ìµœì¢… ë‹µë³€ì„ ì œì‹œí•˜ì„¸ìš”

ìµœì¢… ë‹µë³€:"""

            judge_response = self.judge_llm.generate(judge_prompt, temperature=0.2, max_tokens=200)
            
            if isinstance(judge_response, dict):
                judge_text = judge_response.get("response", "").strip()
            else:
                judge_text = str(judge_response).strip()
                
            total_tokens += len(judge_prompt.split()) + len(judge_text.split())
            print(f"    Judge ì™„ë£Œ")
            
            # ì •í™•ë„ í‰ê°€
            accuracy = self._evaluate_accuracy(judge_text, expected_answer)
            
            return Model7BResult(
                approach="Bì•ˆ 7B (í”„ë¡¬í”„íŠ¸ê°œì„ )",
                question=question,
                draft_responses=draft_responses,
                review_responses=review_responses,
                judge_response=judge_text,
                total_tokens=total_tokens,
                total_time_ms=int((time.time() - start_time) * 1000),
                accuracy_score=accuracy,
                success=True
            )
            
        except Exception as e:
            return Model7BResult(
                approach="Bì•ˆ 7B (í”„ë¡¬í”„íŠ¸ê°œì„ )",
                question=question,
                draft_responses=[],
                review_responses=[],
                judge_response="",
                total_tokens=0,
                total_time_ms=0,
                accuracy_score=0.0,
                success=False,
                error=str(e)
            )
    
    def run_7b_single_baseline(self, question: str, expected_answer: str) -> Model7BResult:
        """7B Single ëª¨ë¸ ë² ì´ìŠ¤ë¼ì¸"""
        
        start_time = time.time()
        
        try:
            print(f"  Single ëª¨ë¸ ì‹¤í–‰...")
            prompt = f"ì§ˆë¬¸: {question}\n\në‹µë³€í•˜ì„¸ìš”:"
            response = self.judge_llm.generate(prompt, temperature=0.3, max_tokens=150)
            
            if isinstance(response, dict):
                answer_text = response.get("response", "").strip()
            else:
                answer_text = str(response).strip()
            
            total_tokens = len(prompt.split()) + len(answer_text.split())
            accuracy = self._evaluate_accuracy(answer_text, expected_answer)
            
            return Model7BResult(
                approach="Single 7B Model",
                question=question,
                draft_responses=[],
                review_responses=[],
                judge_response=answer_text,
                total_tokens=total_tokens,
                total_time_ms=int((time.time() - start_time) * 1000),
                accuracy_score=accuracy,
                success=True
            )
            
        except Exception as e:
            return Model7BResult(
                approach="Single 7B Model",
                question=question,
                draft_responses=[],
                review_responses=[],
                judge_response="",
                total_tokens=0,
                total_time_ms=0,
                accuracy_score=0.0,
                success=False,
                error=str(e)
            )
    
    def _evaluate_accuracy(self, response: str, expected_answer: str) -> float:
        """ì •í™•ë„ í‰ê°€"""
        response_lower = response.lower()
        expected_lower = expected_answer.lower()
        
        # í•µì‹¬ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if expected_lower in response_lower:
            return 1.0
        
        # ë¶€ë¶„ ë§¤ì¹­
        expected_words = set(expected_lower.split())
        response_words = set(response_lower.split())
        
        if not expected_words:
            return 0.0
            
        matching_words = expected_words.intersection(response_words)
        return len(matching_words) / len(expected_words)

def run_7b_model_comparison():
    """7B ëª¨ë¸ ë¹„êµ ì‹¤í—˜"""
    
    print("=" * 80)
    print("7B ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ë¹„êµ ì‹¤í—˜")
    print("=" * 80)
    
    tester = Pipeline7BTester()
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    test_cases = [
        {
            "question": "What is the capital of South Korea?",
            "expected_answer": "Seoul"
        },
        {
            "question": "What is 2+2?",
            "expected_answer": "4"
        },
        {
            "question": "What is the largest planet in our solar system?",
            "expected_answer": "Jupiter"
        },
        {
            "question": "Who wrote Romeo and Juliet?",
            "expected_answer": "Shakespeare"
        },
        {
            "question": "What is the speed of light?",
            "expected_answer": "300000000"  # 3x10^8 m/s
        }
    ]
    
    all_results = []
    
    for i, test_case in enumerate(test_cases):
        question = test_case["question"]
        expected = test_case["expected_answer"]
        
        print(f"\n{i+1}. í…ŒìŠ¤íŠ¸: {question}")
        print("=" * 60)
        
        # Aì•ˆ 7B ì‹¤í–‰
        print("Aì•ˆ 7B (ThoughtAggregator) ì‹¤í–‰ ì¤‘...")
        a_result = tester.run_7b_a_approach(question, expected)
        
        # Bì•ˆ 7B ì‹¤í–‰  
        print("Bì•ˆ 7B (í”„ë¡¬í”„íŠ¸ê°œì„ ) ì‹¤í–‰ ì¤‘...")
        b_result = tester.run_7b_b_approach(question, expected)
        
        # Single 7B ì‹¤í–‰
        print("Single 7B Model ì‹¤í–‰ ì¤‘...")
        single_result = tester.run_7b_single_baseline(question, expected)
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\n--- ê²°ê³¼ ë¹„êµ ---")
        for result in [a_result, b_result, single_result]:
            if result.success:
                print(f"{result.approach}:")
                print(f"  ìµœì¢… ë‹µë³€: {result.judge_response[:100]}...")
                print(f"  ì •í™•ë„: {result.accuracy_score:.2f}")
                print(f"  í† í° ìˆ˜: {result.total_tokens}")
                print(f"  ì‹¤í–‰ ì‹œê°„: {result.total_time_ms}ms")
                print(f"  íš¨ìœ¨ì„±: {result.accuracy_score/result.total_tokens:.6f}")
                
                # Draftë“¤ ìƒ˜í”Œ ì¶œë ¥
                if result.draft_responses:
                    print(f"  Draft ìƒ˜í”Œ: {result.draft_responses[0][:80]}...")
            else:
                print(f"{result.approach}: ì‹¤íŒ¨ - {result.error}")
            print()
        
        all_results.extend([a_result, b_result, single_result])
    
    # ì „ì²´ ê²°ê³¼ ìš”ì•½
    print_7b_summary(all_results)
    
    # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
    save_7b_results(all_results)
    
    return all_results

def print_7b_summary(results: List[Model7BResult]):
    """7B ëª¨ë¸ ê²°ê³¼ ìš”ì•½"""
    
    print("\n" + "=" * 80)
    print("7B ëª¨ë¸ ì‹¤í—˜ ê²°ê³¼ ìš”ì•½")
    print("=" * 80)
    
    # ì ‘ê·¼ë²•ë³„ë¡œ ê·¸ë£¹í™”
    approaches = {}
    for result in results:
        if result.success:
            if result.approach not in approaches:
                approaches[result.approach] = []
            approaches[result.approach].append(result)
    
    print(f"\nğŸ“Š ì„±ëŠ¥ ë¹„êµ:")
    for approach, approach_results in approaches.items():
        if not approach_results:
            continue
            
        avg_accuracy = sum(r.accuracy_score for r in approach_results) / len(approach_results)
        avg_tokens = sum(r.total_tokens for r in approach_results) / len(approach_results)
        avg_time = sum(r.total_time_ms for r in approach_results) / len(approach_results)
        avg_efficiency = avg_accuracy / avg_tokens if avg_tokens > 0 else 0
        
        print(f"\n{approach}:")
        print(f"  í‰ê·  ì •í™•ë„: {avg_accuracy:.3f} ({avg_accuracy*100:.1f}%)")
        print(f"  í‰ê·  í† í° ìˆ˜: {avg_tokens:.1f}")
        print(f"  í‰ê·  ì‹¤í–‰ ì‹œê°„: {avg_time:.1f}ms")
        print(f"  íš¨ìœ¨ì„± (ì •í™•ë„/í† í°): {avg_efficiency:.6f}")
    
    # 0.5B vs 7B ë¹„êµ (ì°¸ê³ ìš©)
    print(f"\nğŸ“ˆ 0.5B ëŒ€ë¹„ ê°œì„  ì˜ˆìƒ:")
    print(f"  ì§€ì‹ ì •í™•ë„: í¬ê²Œ ê°œì„ ë  ê²ƒìœ¼ë¡œ ì˜ˆìƒ")
    print(f"  ì¶”ë¡  ëŠ¥ë ¥: í–¥ìƒë  ê²ƒìœ¼ë¡œ ì˜ˆìƒ") 
    print(f"  ì‹¤í–‰ ì‹œê°„: 5-10ë°° ì¦ê°€")
    print(f"  í† í° íš¨ìœ¨ì„±: Multi-Agentì˜ ì§„ì •í•œ ê°€ì¹˜ í™•ì¸ ê°€ëŠ¥")

def save_7b_results(results: List[Model7BResult]):
    """7B ëª¨ë¸ ê²°ê³¼ ì €ì¥"""
    
    # ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
    serializable_results = []
    for result in results:
        serializable_results.append({
            "approach": result.approach,
            "question": result.question,
            "draft_responses": result.draft_responses,
            "review_responses": result.review_responses,
            "judge_response": result.judge_response,
            "total_tokens": result.total_tokens,
            "total_time_ms": result.total_time_ms,
            "accuracy_score": result.accuracy_score,
            "success": result.success,
            "error": result.error
        })
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨í•œ íŒŒì¼ëª…
    timestamp = int(time.time())
    filename = f"results/7b_pipeline_comparison_{timestamp}.json"
    
    try:
        import os
        os.makedirs("results", exist_ok=True)
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ë¨: {filename}")
    except Exception as e:
        print(f"íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    print("ğŸš€ 7B ëª¨ë¸ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    print("âš ï¸  ì£¼ì˜: 7B ëª¨ë¸ì€ 0.5B ëŒ€ë¹„ ìƒë‹¹íˆ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # qwen2:7b ëª¨ë¸ í™•ì¸
    print("\nğŸ” qwen2:7b ëª¨ë¸ í™•ì¸ ì¤‘...")
    import subprocess
    try:
        result = subprocess.run(["ollama", "list"], capture_output=True, text=True)
        if "qwen2:7b" not in result.stdout:
            print("âŒ qwen2:7b ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("ğŸ“¥ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: ollama pull qwen2:7b")
            exit(1)
        else:
            print("âœ… qwen2:7b ëª¨ë¸ í™•ì¸ë¨")
    except Exception as e:
        print(f"âš ï¸  ollama ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
        print("ìˆ˜ë™ìœ¼ë¡œ 'ollama list'ë¥¼ ì‹¤í–‰í•˜ì—¬ qwen2:7bê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    
    input("\nEnterë¥¼ ëˆŒëŸ¬ì„œ ì‹¤í—˜ì„ ì‹œì‘í•˜ì„¸ìš”...")
    results = run_7b_model_comparison()